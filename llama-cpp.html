

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Using Llama-cpp-python to run LLMs &#8212; Kellogg Research Support Open Source LLM Cookbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'llama-cpp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Example Use Case: 10K Processing" href="use_case.html" />
    <link rel="prev" title="Using Transformers to run LLMs" href="transformers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="welcome.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Kellogg Research Support Open Source LLM Cookbook - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Kellogg Research Support Open Source LLM Cookbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Open Source Large Language Models (LLMs)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Open Source LLMs on KLC</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="slurm_gpu_usage.html">Using GPUs at Northwestern</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Using Transformers to run LLMs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Using Llama-cpp-python to run LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_case.html">Example Use Case: 10K Processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Techniques</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="retrieval.html">Retrieval Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="fine-tuning.html">Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="vision.html">Image Processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="takeaways.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/rs-kellogg/krs-openllm-cookbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rs-kellogg/krs-openllm-cookbook/issues/new?title=Issue%20on%20page%20%2Fllama-cpp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/llama-cpp.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Using Llama-cpp-python to run LLMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-workflow"><strong>Basic Workflow</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-improvements"><strong>Architecture Improvements</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantized-models-what-is-gguf-font"><strong>Quantized Models - What is GGUF?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tradeoffs-between-tools"><strong>Tradeoffs between Tools</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-gemma-model"><strong>Run Gemma model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-mistral-model"><strong>Run Mistral model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-llama2-model"><strong>Run llama2 model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-sources"><strong>Reference Sources</strong></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="using-llama-cpp-python-to-run-llms">
<h1>Using Llama-cpp-python to run LLMs<a class="headerlink" href="#using-llama-cpp-python-to-run-llms" title="Permalink to this heading">#</a></h1>
<p>Much like Tranformers, <code class="docutils literal notranslate"><span class="pre">LLaMa.cpp</span> <span class="pre">(or</span> <span class="pre">LLaMa</span> <span class="pre">C++)</span></code> is a tool or framework to run an open-source LLM. We’ll compare the frameworks and demonstrate how to use the Python wrapper for it: <font color='purple'><strong>llama-cpp-python</strong></font>.</p>
<section id="basic-workflow">
<h2><strong>Basic Workflow</strong><a class="headerlink" href="#basic-workflow" title="Permalink to this heading">#</a></h2>
<p>Arguably, <font color='purple'><strong>llama-cpp-python</strong></font> is simpler to use than the Transformers library for running LLMs. Unlike Transformers, you don’t need to be familiar with PyTorch or the intricacies of token conversion and decoding. <font color='purple'><strong>Llama-cpp-python</strong></font> takes care of these technical details under the hood, allowing you to focus on feeding it text and getting results directly.</p>
<p><strong>1. Load the model:</strong></p>
<p>Load the model in a <font color='purple'><strong>GGUF</strong></font> format <em>(explained in more detail later)</em>.</p>
<p><strong>2. Send your prompt:</strong></p>
<p>Either feed it a plain text prompt or place the query in the appropriate model prompt syntax.</p>
<p><strong>3. Get text response:</strong></p>
<p>Retrieve the text response with no decoding necessary.</p>
</section>
<section id="architecture-improvements">
<h2><strong>Architecture Improvements</strong><a class="headerlink" href="#architecture-improvements" title="Permalink to this heading">#</a></h2>
<p>Since Transformers and <code class="docutils literal notranslate"><span class="pre">LLaMa.cpp</span></code> are both derived from the Transformer architecture, they follow similar processing steps.  However, <code class="docutils literal notranslate"><span class="pre">LLaMa.cpp</span></code> applies some improvements that make it slightly more efficient and easier to run.  The diagram below details the difference between the Llama and Transformer architecture.</p>
<!-- <div style="text-align:center">
    <img src="./images/architectures.png" width="700">
</div> -->
<p><img alt="architects" src="_images/architectures.png" /></p>
<p>The upshot of these tweaks are:</p>
<ul class="simple">
<li><p><strong>Better training</strong>: LLaMA uses a technique called <strong>pre-normalization</strong> to improves training stability by adjusting the data before processing it. Consider this akin to tuning up your guitar before each song to ensure the strings are at the right tension for consistent sound.</p></li>
</ul>
<!-- This is similar to how a chef might preheat an oven for even cooking. -->
<ul class="simple">
<li><p><strong>Faster calculations</strong>: LLaMA utilizes the <strong>SwiGLU activation function</strong> to help the model compute information faster. This is like using a pick with a special grip that makes it easier and faster to strum chords or play single notes.</p></li>
</ul>
 <!-- This is like a sharper knife for quicker chopping. -->
<ul class="simple">
<li><p><strong>Simpler positioning</strong>: LLaMA incorporates <strong>rotary embeddings</strong> which track word order in a more efficient way compared to absolute positional embeddings. Imagine that the frets at different positions of your guitar neck were color-coded instead of just metal bars. This makes it quicker to find the right notes on the fretboard.</p></li>
</ul>
 <!-- Imagine using color-coded ingredients (rotary) instead of labels (absolute) to keep track in a recipe--></section>
<section id="quantized-models-what-is-gguf-font">
<h2><strong>Quantized Models - What is GGUF?</strong></font><a class="headerlink" href="#quantized-models-what-is-gguf-font" title="Permalink to this heading">#</a></h2>
<p>In order to run an LLM, <font color='purple'><strong>llama-cpp-python</strong></font> requires the model’s parameters – the building blocks of its knowledge. These are stored as complex, multidimensional arrays of values called <strong>tensors</strong>. In machine learning a tensor shows the values of nodes in a neural network.</p>
<p><img alt="tensor" src="_images/tensor.png" /></p>
<p>The data type chosen for these tensors (like Float64, Float16, or even integers) impacts both accuracy and memory usage. You can think of the data type as the number of “digits” used to represent the information in memory:</p>
<ul class="simple">
<li><p><strong>Higher precision data types (e.g., Float64)</strong>: Offer greater accuracy and stability during training, but require more memory and computational resources.</p></li>
<li><p><strong>Lower precision data types (e.g., Float16)</strong>: Reduce memory requirements and potentially speed up computations, but might introduce slight accuracy trade-offs.</p></li>
</ul>
<p><font color='purple'><strong>Llama-cpp-python</strong></font> simplifies how to store model inputs by using a single, compressed format called <font color='purple'><strong>GGUF (GPT-Generated Unified Format)</strong></font>. <font color='purple'><strong>GGUF</strong></font> can leverage a technique called <font color='purple'><strong>quantization</strong></font> to further optimize storage. <font color='purple'><strong>Quantization</strong></font> essentially “compresses” the data by representing the model weights using less precise data types. The image below demonstrates how <font color='purple'><strong>quantization</strong></font> works:</p>
<!--<div style="text-align:center">
    <img src="./images/quantize.gif" width="800">
</div> -->
<p><img alt="quantize" src="_images/quantize.gif" /></p>
<p>In practice <font color='purple'><strong>quantization</strong></font> can entail moving from a float 16 (FP16) data type to an integer 4 (INT4) data type.</p>
<p>By using <font color='purple'><strong>quantization</strong></font>, <font color='purple'><strong>llama-cpp-python</strong></font> makes it easier to work with LLMs on devices with limited memory or processing power. Here you see how <font color='purple'><strong>quantization</strong></font> reduces the size of the llama2 models you can call.  This makes it feasible to run these models on a single GPU or even CPUs.</p>
<!--<div style="text-align:center">
    <img src="./images/llama_size.png" width="800">
</div>-->
<p><img alt="llama" src="_images/llama_size.png" /></p>
<p>While you can create your own <font color='purple'><strong>GGUF</strong></font> files for most models, it’s also possible to download these files from Hugging Face. The <font color='purple'><strong>GGUF</strong></font> files for llama2 can be found here: <a class="reference external" href="https://huggingface.co/TheBloke/Llama-2-7B-GGUF">https://huggingface.co/TheBloke/Llama-2-7B-GGUF</a>.</p>
</section>
<section id="tradeoffs-between-tools">
<h2><strong>Tradeoffs between Tools</strong><a class="headerlink" href="#tradeoffs-between-tools" title="Permalink to this heading">#</a></h2>
<p>Even though the architecture and basic steps suggest that <font color='purple'><strong>llama-cpp-python</strong></font> might be easier to run and more efficient than the python Transformers library, the improvements come with some tradeoffs.</p>
<ul class="simple">
<li><p><strong>Inference only</strong>: <font color='purple'><strong>llama-cpp-python</strong></font> is primarily for running inference (using the model to generate text). Transformers offer more comprehensive functionality for model development and experimentation, which makes it easier to fine-tune or perform RAG using Transformers.</p></li>
<li><p><strong>Slower for small models</strong>: lla <font color='purple'><strong>llama-cpp-python</strong></font> might be slower for very small GGUF files. The overhead of using  <font color='purple'><strong>llama-cpp-python</strong></font> might outweigh the benefits for tiny models. Transformers might be a better choice in such cases.</p></li>
<li><p><strong>Precision loss</strong>: Quantization does introduce some loss in precision. This might be negligible for some tasks, but Transformers with full-precision computation could be preferable for others.</p></li>
</ul>
<p><font color='purple'><em>Choose <strong>Transformers</strong> for:</em></font></p>
<ul class="simple">
<li><p>Fine-tuning models</p></li>
<li><p>Using RAG or other advanced techniques</p></li>
<li><p>Working with very small models</p></li>
<li><p>When high precision is crucial</p></li>
</ul>
<p><font color='purple'><em>Choose <strong>llama_cpp_python</strong> for:</em></font></p>
<ul class="simple">
<li><p>Faster inference on large models</p></li>
<li><p>Deploying models on CPUs</p></li>
</ul>
</section>
<section id="run-gemma-model">
<h2><strong>Run Gemma model</strong><a class="headerlink" href="#run-gemma-model" title="Permalink to this heading">#</a></h2>
<p>Here is some sample code to run a single prompt with the gemma 7B instruct model:</p>
<p><font color='purple'><strong>Python script</strong></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">###########################################</span>
<span class="c1"># Gemma model example with llama_cpp_python</span>
<span class="c1">###########################################</span>
<span class="c1"># libraries</span>
<span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>

<span class="c1">#########</span>
<span class="c1"># Inputs</span>
<span class="c1">#########</span>

<span class="c1"># model</span>
<span class="n">model_path</span> <span class="o">=</span><span class="s2">&quot;/kellogg/software/llama_cpp/model/gemma-7b-it.gguf&quot;</span>

<span class="c1"># settings</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">threads</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">gpu_layers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">max_tokens_select</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">temperature_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">0</span>
<span class="n">top_p_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span>
<span class="n">top_k_select</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
<span class="n">include_prompt</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Prompts</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;What kind of pickups are on an ESP LTD Alexi Ripped?&quot;</span>

<span class="c1"># Prompt Syntax (prompt written in gemma prompt syntax)</span>
<span class="n">prompt_sytnax</span> <span class="o">=</span> <span class="s2">&quot;&lt;start_of_turn&gt;user&quot;</span> <span class="o">+</span> <span class="n">prompt</span> <span class="o">+</span> <span class="s2">&quot;&lt;end_of_turn&gt;&quot;</span> <span class="o">+</span> <span class="s2">&quot;&lt;start_of_turn&gt;model&quot;</span>

<span class="c1">#####################</span>
<span class="c1"># LLaMa.cpp arguments</span>
<span class="c1">#####################</span>

<span class="c1"># load model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span>
  <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>  <span class="c1"># The path to the model file</span>
  <span class="n">n_ctx</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>  <span class="c1"># The max sequence length to use - adjust based on your model&#39;s requirements</span>
  <span class="n">n_threads</span><span class="o">=</span><span class="n">threads</span><span class="p">,</span>  <span class="c1"># The number of CPU threads to use</span>
  <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">gpu_layers</span>  <span class="c1"># Set to 0 if you want to use CPU only and -1 if you want to use all available GPUs</span>
<span class="p">)</span>

<span class="c1"># send prompt</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens_select</span><span class="p">,</span> 
    <span class="n">temperature</span><span class="o">=</span><span class="n">temperature_select</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="n">top_p_select</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="n">top_k_select</span><span class="p">,</span>
    <span class="n">echo</span> <span class="o">=</span> <span class="n">include_prompt</span>
    <span class="p">)</span>

<span class="c1">##############</span>
<span class="c1"># Get Response</span>
<span class="c1">##############</span>
<span class="n">response_text</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Breaking down the parameter options:</p>
<p><strong>At the model load step</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_path</span></code> is the path to the model file being used. Note that you can download this file from Hugging Face here: <a class="reference external" href="https://huggingface.co/google/gemma-7b-it/tree/main">https://huggingface.co/google/gemma-7b-it/tree/main</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_ctx</span></code> is the context window for the model; max number of tokens in the prompt; default is 512</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_threads</span></code> is the number of the number of CPU threads</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_gpu_layers</span></code>set to 0 if you want to use CPU only and -1 if you want to use all available GPUs</p></li>
</ul>
<p><strong>At the model output step</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prompt</span></code> is the input prompt for the model. Under the hood, the text is tokenized and passed to the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code> is the maximum number of tokens to be generated in the model’s response</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code> this value ranges from 0 to 1. The lower the value, the more deterministic the end result. A higher value leads to more randomness.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_p</span></code> is used to control the diversity of the predictions, meaning that it selects the most probable tokens whose cumulative probability exceeds a given threshold. Starting from zero, a higher value increases the chance of finding a better output but requires additional computations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">echo</span></code> specifies a boolean used to determine whether the model includes the original prompt at the beginning (True) or does not include it (False).</p></li>
</ul>
<p><em>For more information about these parameters and additional ones, please see: <a class="reference external" href="https://llama-cpp-python.readthedocs.io/en/latest/api-reference/">https://llama-cpp-python.readthedocs.io/en/latest/api-reference/</a>.</em></p>
<p>You can find this file here: <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/gemma_ex.py">scripts/llama_cpp_python/gemma_ex.py</a>.  This file is saved as <strong>gemma_ex.py</strong> on KLC here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">kelloggs</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">llama_cpp</span><span class="o">/</span><span class="n">code</span>
</pre></div>
</div>
<p><font color='purple'><strong>SLURM script</strong></font></p>
<p>You can run this file with the following SLURM script:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --account=e32337</span>
<span class="c1">#SBATCH --partition=gengpu</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --gres=gpu:a100:1</span>
<span class="c1">#SBATCH --constraint=pcie</span>
<span class="c1">#SBATCH --time 0:30:00</span>
<span class="c1">#SBATCH --mem=40G</span>

<span class="n">module</span> <span class="n">purge</span>
<span class="n">module</span> <span class="n">use</span> <span class="o">/</span><span class="n">kellogg</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">Modules</span><span class="o">/</span><span class="n">modulefiles</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">llama_cpp</span><span class="o">/</span><span class="mf">2.38</span>

<span class="n">python3</span> <span class="n">gemma_ex</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
</div>
<p>You can find this file here: <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/gemma_ex.sh">scripts/llama_cpp_python/gemma_ex.sh</a>. This file is also saved to the same directory provided above as <strong>gemma_test.sh</strong>. You can run this shell script with:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="n">gemma_test</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can see how to run the code on Quest in <a class="reference external" href="https://kellogg-shared.s3.us-east-2.amazonaws.com/videos/llama-cpp-python.mov">this video</a></p>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 19 key-value pairs and 254 tensors from /kellogg/software/llama_cpp/models/gemma-7b-it.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-7b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                          gemma.block_count u32              = 28
llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 3072
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16
llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [&quot;&lt;pad&gt;&quot;, &quot;&lt;eos&gt;&quot;, &quot;&lt;bos&gt;&quot;, &quot;&lt;unk&gt;&quot;, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - type  f32:  254 tensors
llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256128
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 3072
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_rot            = 192
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 24576
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = all F32 (guessed)
llm_load_print_meta: model params     = 8.54 B
llm_load_print_meta: model size       = 31.81 GiB (32.00 BPW)
llm_load_print_meta: general.name     = gemma-7b-it
llm_load_print_meta: BOS token        = 2 &#39;&lt;bos&gt;&#39;
llm_load_print_meta: EOS token        = 1 &#39;&lt;eos&gt;&#39;
llm_load_print_meta: UNK token        = 3 &#39;&lt;unk&gt;&#39;
llm_load_print_meta: PAD token        = 0 &#39;&lt;pad&gt;&#39;
llm_load_print_meta: LF token         = 227 &#39;&lt;0x0A&gt;&#39;
llm_load_tensors: ggml ctx size =    0.19 MiB
llm_load_tensors: offloading 28 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 29/29 layers to GPU
llm_load_tensors:        CPU buffer size =  3001.50 MiB
llm_load_tensors:      CUDA0 buffer size = 29568.67 MiB
............................................................................................
llama_new_context_with_model: n_ctx	 = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:	  CUDA0 KV buffer size =   224.00 MiB
llama_new_context_with_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     8.01 MiB
llama_new_context_with_model:	   CUDA0 compute buffer size =   109.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =   512.25 MiB
llama_new_context_with_model: graph splits (measure): 4
AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = $
Model metadata: {&#39;tokenizer.ggml.unknown_token_id&#39;: &#39;3&#39;, &#39;tokenizer.ggml.padding_token_id&#39;: &#39;0&#39;, &#39;tokenizer.ggml.eos_token_id&#39;: &#39;1&#39;, &#39;general.architecture&#39;: &#39;gemma&#39;, &#39;g$

llama_print_timings:        load time =    1732.80 ms
llama_print_timings:	  sample time =      74.24 ms /    34 runs   (    2.18 ms per token,   457.97 tokens per second)
llama_print_timings: prompt eval time =    1732.72 ms /    15 tokens (  115.51 ms per token,     8.66 tokens per second)
llama_print_timings:        eval time =    8929.80 ms /    33 runs   (  270.60 ms per token,     3.70 tokens per second)
llama_print_timings:	   total time =   11326.02 ms /    48 tokens


The Esp Ltd.AlexI Ripper has ceramic humbuckers with coil tapping capabilities, which give you a wide range from clean to overdrive tones and everything in between

</pre></div>
</div>
<p>Likewise, you can modify the python file to run multiple prompts.  The example script below is saved as <strong>gemma_workflow.py</strong> in the same directory listed above.  You can also find it here: <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/gemma_workflow.py">scripts/llama_cpp_python/gemma_workflow.py</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">######################################</span>
<span class="c1"># Gemma Workflow with llama_cpp_python</span>
<span class="c1">######################################</span>
<span class="c1"># libraries</span>
<span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1">#########</span>
<span class="c1"># Inputs</span>
<span class="c1">#########</span>

<span class="c1"># model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;/kellogg/software/llama_cpp/models/gemma-7b-it.gguf&quot;</span>

<span class="c1"># prompts</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;What is the main idea of Guy Debord&#39;s Societe du Spectacle?&quot;</span><span class="p">,</span> 
           <span class="s2">&quot;What kind of pickups are on an ESP LTD Alexi Ripped?&quot;</span><span class="p">,</span> 
           <span class="s2">&quot;How does Allama Iqbal&#39;s concept of the khudi relate to Nietzsche&#39;s Ubermensch?&quot;</span>
<span class="p">]</span>

<span class="c1"># output</span>
<span class="n">output_file</span> <span class="o">=</span> <span class="s2">&quot;/kellogg/software/llama_cpp/output/gemma_test.csv&quot;</span>

<span class="c1"># settings</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># The max sequence length to use - adjust based on your model&#39;s requirements</span>
<span class="n">threads</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># The number of CPU threads to use</span>
<span class="n">gpu_layers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># Set to 0 if you want to use CPU only and -1 if you want to use all available GPUs</span>
<span class="n">max_tokens_select</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">temperature_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">0</span> 
<span class="n">top_p_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span>
<span class="n">top_k_select</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
<span class="n">include_prompt</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1">############</span>
<span class="c1"># Functions</span>
<span class="c1">############</span>

<span class="c1"># get prompt response</span>
<span class="k">def</span> <span class="nf">get_completion</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens_select</span><span class="p">,</span> <span class="n">temperature_select</span><span class="p">,</span> <span class="n">top_p_select</span><span class="p">,</span> <span class="n">top_k_select</span><span class="p">,</span> <span class="n">include_prompt</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>

    <span class="c1"># send prompt</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span>
      <span class="n">prompt</span><span class="p">,</span>
      <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens_select</span><span class="p">,</span> 
      <span class="n">temperature</span><span class="o">=</span><span class="n">temperature_select</span><span class="p">,</span>
      <span class="n">top_p</span><span class="o">=</span><span class="n">top_p_select</span><span class="p">,</span>
      <span class="n">top_k</span><span class="o">=</span><span class="n">top_k_select</span><span class="p">,</span>
      <span class="n">echo</span> <span class="o">=</span> <span class="n">include_prompt</span><span class="p">)</span>
    
    <span class="c1"># get response</span>
    <span class="n">response_text</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">response_text</span>
  
  <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="c1"># save results to a df</span>
<span class="k">def</span> <span class="nf">save_results</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">run_time</span><span class="p">):</span>

    <span class="c1"># create empty df</span>
    <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">,</span> <span class="s1">&#39;response&#39;</span><span class="p">,</span> <span class="s1">&#39;run_time&#39;</span><span class="p">])</span>
    
    <span class="c1"># create df from current row</span>
    <span class="n">row_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;prompt&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">prompt</span><span class="p">],</span>
        <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">],</span>
        <span class="s1">&#39;run_time&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">run_time</span><span class="p">]</span>
    <span class="p">})</span>
    
    <span class="c1"># combine</span>
    <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">row_df</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># return dataframe</span>
    <span class="k">return</span> <span class="n">results_df</span>

<span class="c1">######</span>
<span class="c1"># RUN</span>
<span class="c1">######</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
   <span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>  
               <span class="n">n_ctx</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>  
               <span class="n">n_threads</span><span class="o">=</span><span class="n">threads</span><span class="p">,</span>  
               <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">gpu_layers</span><span class="p">)</span>

   <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
        <span class="c1"># run</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">max_tokens_select</span><span class="p">,</span> <span class="n">temperature_select</span><span class="p">,</span> <span class="n">top_p_select</span><span class="p">,</span> <span class="n">top_k_select</span><span class="p">,</span> <span class="n">include_prompt</span><span class="p">)</span>
        <span class="n">run_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

        <span class="c1"># print results</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;========================&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run Time: </span><span class="si">{</span><span class="n">run_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;========================&quot;</span><span class="p">)</span>

        <span class="c1"># save progress</span>
        <span class="n">results_df</span> <span class="o">=</span> <span class="n">save_results</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">run_time</span><span class="p">)</span>
        <span class="n">results_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>.....
llama_print_timings:        load time =    5117.72 ms
llama_print_timings:      sample time =     910.97 ms /   320 runs   (    2.85 ms per token,   351.28 tokens per second)
llama_print_timings: prompt eval time =    5116.70 ms /    18 tokens (  284.26 ms per token,     3.52 tokens per second)
llama_print_timings:        eval time =   85380.73 ms /   319 runs   (  267.65 ms per token,     3.74 tokens per second)
llama_print_timings:       total time =   96673.56 ms /   337 tokens
========================
Prompt: What is the main idea of Guy Debord&#39;s Societe du Spectacle?
Response: 

The Society of Control (1973) argues that in advanced capitalist societies, social life itself has become a spectacle. This society rests on two pillars: control and simulation—the latter being more powerful than even economic power because it controls our perception as individuals as well as our understanding as collective beings.&lt;br&gt; 
Guy debord argued against this view claiming instead for an understanding rooted firmly within historical materialism.&lt;br&gt; 
Debord argued against seeing contemporary capitalism as primarily concerned with production for profit but rather with production for consumption.&lt; He argued this shift was evident not only from changes to labor practices but also from changes to consumer behavior.&lt;br&gt; 
Therefore his primary concern was not with individual perception but with collective behavior.&lt; He argued this shift was evident not only from changes to labor practices but also from changes to consumer behavior.&lt;br&gt; 
Run Time: 96.674813747406
========================

llama_print_timings:        load time =    5117.72 ms
llama_print_timings:      sample time =     107.74 ms /    34 runs   (    3.17 ms per token,   315.57 tokens per second)
llama_print_timings: prompt eval time =    1299.31 ms /    13 tokens (   99.95 ms per token,    10.01 tokens per second)
llama_print_timings:        eval time =    8821.49 ms /    33 runs   (  267.32 ms per token,     3.74 tokens per second)
llama_print_timings:       total time =   10758.35 ms /    46 tokens

========================
Prompt: What kind of pickups are on an ESP LTD Alexi Ripped?
Response: 

The Esp Ltd.AlexI Ripper has ceramic humbuckers with coil tapping capabilities, which give you a wide range from clean to overdrive tones and everything in between
Run Time: 10.760140419006348

========================

llama_print_timings:        load time =    5268.39 ms
llama_print_timings:      sample time =    1060.79 ms /   377 runs   (    2.81 ms per token,   355.40 tokens per second)
llama_print_timings: prompt eval time =    1910.34 ms /    20 tokens (   95.52 ms per token,    10.47 tokens per second)
llama_print_timings:        eval time =   99700.63 ms /   376 runs   (  265.16 ms per token,     3.77 tokens per second)
llama_print_timings:       total time =  108664.66 ms /   396 tokens
========================
Prompt: How does Allama Iqbal&#39;s concept of the khudi relate to Nietzsche&#39;s Ubermensch?
Response: 

Allama MuhammadIqbal and FriedrichNietzsche were two prominent philosophers who explored similar themes in their respective works. One such theme is that  of self-cultivation, which they referred as Khudī (Urdu) or Übermenschen(German). 


**Khüdî by Allaima IQBAL:**
In his poetry collection &quot;The Secrets,&quot; Allaima IQBAL argued for a spiritual awakening through individual effort: 
&quot;Khüdî hai har insān ka apna,&quot; meaning &quot;Selfhood&quot; or &quot;Individuality&quot; for every human being.&quot; 
He believed individuals should cultivate inner strength through moral development rather than material possessions or external validation . 


**Übermenschen by NIETZSCHE:**
Friedrich NIETZSCHE proposed a different approach towards selffactualization: through overcoming limitations rather than conforming with societal norms:
&quot;The Übermenschen... are those individuals whose spiritual strength has overcome all limitations.&quot;
He emphasized individual strength over conformity , urging humans not be afraid challenge existing structures .


**Comparison:**
Both concepts emphasize individual empowerment through spiritual awakening or overcoming limitations . However , there are some key differences between them:
* **Emphasis:**
 - **IQBAL:** Focuses on moral development through individual effort
 - **NIETZSCHE:** Emphasizes spiritual strength through overcoming limitations
* **Approach:**
 - **IQBAL:** Advocates for spiritual awakening through moral development
 - **NIETZSCHE:** Encourages overcoming limitations through individual strength
* **Cultural Context:**
 - **IQBAL:** Rooted within South Asian cultural values
 - **NIETZSCHE:** Developed within European philosophical tradition

**Conclusion:**
While both concepts share similarities , each philosopher offers a unique perspective on individual empowerment through spiritual awakening or overcoming limitations . IQBAL emphasizes moral development within a cultural context , while NIETZSCHE encourages individual strength beyond societal norms .
Run Time: 108.66637086868286

</pre></div>
</div>
</section>
<section id="run-mistral-model">
<h2><strong>Run Mistral model</strong><a class="headerlink" href="#run-mistral-model" title="Permalink to this heading">#</a></h2>
<p>Please see <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/mistral_ex.py">scripts/llama_cpp_python/misral_ex.py</a>.</p>
<p>Note that you can download this file from Hugging Face here: <a class="reference external" href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF">https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF</a></p>
</section>
<section id="run-llama2-model">
<h2><strong>Run llama2 model</strong><a class="headerlink" href="#run-llama2-model" title="Permalink to this heading">#</a></h2>
<p>Please see <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/llama2_ex.py">scripts/llama_cpp_python/llama2_ex.py</a>.</p>
<p>Note that you can download this file from Hugging Face here:  <a class="reference external" href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF">https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF</a></p>
</section>
<section id="reference-sources">
<h2><strong>Reference Sources</strong><a class="headerlink" href="#reference-sources" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/ggerganov/llama.cpp">Llama.cpp Git Repo</a></p></li>
<li><p><a class="reference external" href="https://github.com/abetlen/llama-cpp-python">Llama.cpp.python Git Repo</a></p></li>
<li><p><a class="reference external" href="https://www.datacamp.com/tutorial/llama-cpp-tutorial">Llama.cpp Tuturial</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;metechsolutions/llm-by-examples-use-gguf-quantization-3e2272b66343">LLM By Examples — Use GGUF Quantization</a></p></li>
<li><p><a class="reference external" href="https://www.tensorops.ai/post/what-are-quantized-llms">What are Quantized LLMs</a></p></li>
<li><p><a class="reference external" href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html">Quantize Llama Models with GGUF adn llama.cpp</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;ingridwickstevens/quantization-of-llms-with-llama-cpp-9bbf59deda35">Quantization of LLMs with llama.cpp</a></p></li>
<li><p><a class="reference external" href="https://www.promptingguide.ai/">Prompt Engineering Guide</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="transformers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Using Transformers to run LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="use_case.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Example Use Case: 10K Processing</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-workflow"><strong>Basic Workflow</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-improvements"><strong>Architecture Improvements</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantized-models-what-is-gguf-font"><strong>Quantized Models - What is GGUF?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tradeoffs-between-tools"><strong>Tradeoffs between Tools</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-gemma-model"><strong>Run Gemma model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-mistral-model"><strong>Run Mistral model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-llama2-model"><strong>Run llama2 model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-sources"><strong>Reference Sources</strong></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kellogg Research Support
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>