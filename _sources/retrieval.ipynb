{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>__Retrieval Augmented Generation__</font>\n",
    "\n",
    "<font color='purple'>Retrieval Augmented Generation (RAG)</font> is a powerful paradigm in natural language processing that combines the strengths of information retrieval and language generation. This approach involves retrieving relevant information from a large dataset and using that information to enhance the generation of accurate text.  \n",
    "\n",
    "The phrase <font color='purple'>Retrieval Augmented Generation</font> comes from a recent [paper by Lewis et al. from Facebook AI](https://research.facebook.com/publications/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks/). The idea is to use a pre-trained language model (LM) to generate text, but to use a separate retrieval system to find relevant documents to condition the LM on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>How it Works</font>\n",
    "\n",
    "From start to finish, the RAG relies on 5 steps:\n",
    "![RAG steps](./images/rag.png)  \n",
    "\n",
    "**1. Load**  \n",
    "Load documents from different source files (_url, csv, pdf, txt_) in diverse locations (s3 storage, public sites, etc.)  \n",
    "\n",
    "**2. Transform**  \n",
    "Prepare larger documents for retrieval by creating splits or chunks the data.  \n",
    "\n",
    "**3. Embed**  \n",
    "Create embeddings for documents to capture the semantic meaning of the text. This later enables models to efficiently find other pieces of text that are similar.  \n",
    "\n",
    "**4. Store**  \n",
    "Vector stores support efficient storage and search of document embeddings.  \n",
    "\n",
    "**5. Retrieve**  \n",
    "Relevant information is retrieved to produce more informed and context-aware responses.\n",
    "\n",
    "During runtime, this blending of retrieval and generation enhances the richness and relevance of the generated content.\n",
    "![RAG runtime](./images/basic_rag.png)  \n",
    "\n",
    "_Taken from: [https://docs.llamaindex.ai/en/stable/_static/getting_started/basic_rag.png]([https://docs.llamaindex.ai/en/stable/_static/getting_started/basic_rag.png])_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Sample Uses Cases</font>\n",
    "\n",
    "- **Question Answering Systems or Conversational Agents**:  \n",
    "Retrieve information from vast knowledge bases, such as multiple pdf or csv files, and incorporate into response (example today).\n",
    "\n",
    "- **Context Creation**:  \n",
    "Enhance the generation of informative text by pulling in relevant details from a wide range of sources.\n",
    "\n",
    "- **Code Generation**:  \n",
    "Assist in generating code snippets by retrieving information from programming knowledge bases. \n",
    "\n",
    "- **Prevent Hallucinations**:  \n",
    "Bring in external knowledge to check whether a GPT response is a hallucination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Example: Retrieving Information Non-existent in Training</font>\n",
    "\n",
    "One way to use RAG is to feed the LLM with up-to-date information.  \n",
    "The Llama 2 was trained between January 2023 and July 2023. The Mistral 7B model was released in September 2023. Let's ask LLama2 a question about the Mistral 7B model.\n",
    "\n",
    "```\n",
    "# Run the query through Llama2 13B chat model with test_llama2.py  \n",
    "query = \"[INST]What is a Mistral 7B language model?[/INST]\"\n",
    "```\n",
    "\n",
    "__Output:__  \n",
    "> [INST]What is a Mistral 7B language model?[/INST]  I'm not familiar with a \"Mistral 7B language model.\" It's possible that this is a custom or proprietary language model developed by a specific organization or individual, and not a widely known or used model.\n",
    "> \n",
    "> There are many language models available, each with their own strengths and weaknesses, and it's important to choose the right model for your specific use case. Some popular language models include BERT, RoBERTa, XLNet, and transformers. These models have been pre-trained on large datasets and can be fine-tuned for specific tasks such as sentiment analysis, question answering, and text classification.\n",
    "> \n",
    "> If you have any more information about the Mistral 7B language model, such as its capabilities, performance, or the organization that developed it, I may be able to provide more assistance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The release of Mistral 7B language model can be found here: https://mistral.ai/news/announcing-mistral-7b/\n",
    "\n",
    "![Selenium doc](./images/mistral.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, we can use RAG to feed details about the release note. The code below will take the contents of a webpage and follow the 5 steps outlined above to retrieve the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load libraries\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# === Download web content\n",
    "url_link = \"https://mistral.ai/news/announcing-mistral-7b/\"\n",
    "response = requests.get(url_link)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "webpage_content = soup.get_text().strip()\n",
    "data_folder = Path(\"./test_data\")\n",
    "if not data_folder.exists():\n",
    "    data_folder.mkdir()\n",
    "txt_file = data_folder / \"webpage_content.txt\"\n",
    "with open(txt_file, \"w\") as f:\n",
    "    f.write(webpage_content)\n",
    "\n",
    "# Settings for embedding model\n",
    "embedding_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "embedding_chunk_size = 512\n",
    "embed_model = HuggingFaceEmbedding(model_name=embedding_name, max_length=512)\n",
    "\n",
    "# Settings for LLM\n",
    "LLAMA2_13B_CHAT = \"/kellogg/data/llm_models_opensource/llama2_meta_huggingface/models--meta-llama--Llama-2-13b-chat-hf/snapshots/29655417e51232f4f2b9b5d3e1418e5a9b04e80e\"\n",
    "selected_model = LLAMA2_13B_CHAT\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    tokenizer_name=selected_model,\n",
    "    model_name=selected_model,\n",
    "    device_map=\"auto\",\n",
    "    # generate_kwargs={\"temperature\": 0.5, \"top_p\": 0.9, \"top_k\": 2, \"do_sample\": True},\n",
    ")\n",
    "\n",
    "# === 1. Load\n",
    "documents = SimpleDirectoryReader(data_folder).load_data()\n",
    "\n",
    "# Set up FAISS vector store\n",
    "d = 1024 # embedding dimension\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    # === 2. Transform\n",
    "    transformations=[SentenceSplitter(chunk_size=embedding_chunk_size)],\n",
    "    # === 3. Embed\n",
    "    embed_model=embed_model,\n",
    "    # === 4. Store\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "# === 5. Retrieve\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "print(\"====================================\")\n",
    "print(f\"Selected LLM: {selected_model}\")\n",
    "query = \"[INST]What is a Mistral 7B language model?[/INST]\"\n",
    "response = query_engine.query(query)\n",
    "print(\"====================================\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Response: \")\n",
    "print(response)\n",
    "print(\"====================================\")\n",
    "\n",
    "print(f\"Execution time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output:__  \n",
    "```\n",
    "Selected LLM: /kellogg/data/llm_models_opensource/llama2_meta_huggingface/models--meta-llama--Llama-2-13b-chat-hf/snapshots/29655417e51232f4f2b9b5d3e1418e5a9b04e80e\n",
    "\n",
    "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
    "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.66it/s]\n",
    "====================================\n",
    "Query: [INST]What is a Mistral 7B language model?[/INST]\n",
    "Response: \n",
    "Based on the provided context information, a Mistral 7B language model is a powerful language model developed by Mistral AI. It is a 7.3 billion parameter model that outperforms other 7B models on various benchmarks and approaches the performance of CodeLlama 7B on code tasks while remaining good at English tasks. The model is released under the Apache 2.0 license and can be used without restrictions. It is easy to fine-tune for any task and has been demonstrated to outperform Llama 2 13B chat.\n",
    "====================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>More sample scripts</font>  \n",
    "More sample scripts can be found at the [scripts/rag](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/rag) folder of our github repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
