{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "<font color='purple'>__Fine-tuning__</font> takes large language models beyond basic prompting. It allows you to train the model on a larger dataset specifically tailored to your desired task, leading to several advantages:\n",
    "\n",
    "* ___Superior Results___: Compared to prompting, fine-tuning delivers higher quality outputs because the model learns directly from a wider range of examples.\n",
    "\n",
    "* ___Efficiency___: You can train on more data than can fit in a single prompt, saving on token usage and reducing latency for requests.\n",
    "\n",
    "* ___Specificity___: Fine-tuning excels at tasks requiring specific styles, tones, formats, or consistent outputs. It enhances reliability in achieving your desired results, handles complex instructions better, and tackles edge cases effectively.\n",
    "\n",
    "* ___New Skills___: Fine-tuning empowers the model to master new skills or complete tasks that are difficult to articulate through simple prompts.\n",
    "\n",
    "### <font color='purple'>_Tradeoffs between Techniques_</font>\n",
    "\n",
    "Before delving into the mechanics of fine-tuning; note that it might not always be the best solution.  Here's a breakdown of when to apply prompt-engineering, RAG, or fine-tuning:\n",
    "\n",
    "* <font color='purple'>_Use Prompt Engineering for_</font>:\n",
    "\n",
    "    * Achieving good results for simpler tasks without extensive training data.\n",
    "    * Breaking down complex tasks into smaller, sequential prompts for the model to follow (prompt chaining).\n",
    "\n",
    "* <font color='purple'>_Use Retrieval-Augmented Generation (RAG) for_</font>:\n",
    "\n",
    "    * Situations with a large, relevant document database.\n",
    "    * Tasks requiring factual accuracy where context retrieval is beneficial.\n",
    "\n",
    "* <font color='purple'>_Use Fine-Tuning for_</font>:\n",
    "\n",
    "    * Tasks requiring consistent outputs in a particular style, tone, or format.\n",
    "    * Achieving specific behaviors from the model.\n",
    "    * Mastering complex tasks that are difficult to express through simple prompts.\n",
    "\n",
    "Our advice is to experiment with crafting effective instructions and prompts for the model before attempting fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>_Load and Test Original Model_</font>\n",
    "\n",
    "In this workshop, we'll demonstrate how to fine-tune a Mistral model. Mistral is especially apt for performing classification tasks. \n",
    "\n",
    "The first step in fine-tuning should always be to properly load and test the out-of-the-box model. The code below loads a quantized Mistral model using Transformers.  It uses the first text example from the _AG News_ dataset on Hugging Face here: https://huggingface.co/datasets/ag_news.  It asks the model to classify this text into 1 of 4 categories:\n",
    "\n",
    "* 0 - World News\n",
    "* 1 - Sports\n",
    "* 2 - Business\n",
    "* 3 - Science and Technology.\n",
    "\n",
    "It places this prompt in the Mistral __prompt syntax__.  You can find `query_initial.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Fine Tune - Initial Query\n",
    "#############################\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# model paths\n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/mistral_mistralAI\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# quantized model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir=llm_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True, cache_dir=llm_dir)\n",
    "\n",
    "# query function\n",
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <s>\n",
    "  [INST]\n",
    "  Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech.\n",
    "  {query}\n",
    "  [/INST]\n",
    "  </s>\n",
    "  <s>\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encodeds.to(device)\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])\n",
    "\n",
    "# query\n",
    "query = \"\"\"\n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\"\"\"\n",
    "\n",
    "# submit query\n",
    "result = get_completion(query=query, model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output:__  \n",
    "```\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.30s/it]\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.32s/it]\n",
    "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "<s> \n",
    "  <s> \n",
    "  [INST]\n",
    "  Below is a text from AG News dataset. Classify it into one of the four classes: World, Sports, Business, Sci/Tech.\n",
    "  \n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\n",
    "  [/INST]\n",
    "  </s> \n",
    "  <s> \n",
    "\n",
    "  </s> This text can be classified as: Business.\n",
    "\n",
    "  The reason for this classification is that the text discusses \"Wall Street's dwindling band of ultra-cynics\" and \"short-sellers\" who are \"seeing green again.\" These terms are typically associated with finance and investing, which falls within the Business category. The mention of Wall Street is also a strong indicator of a Business article.\n",
    "\n",
    "  Please note that even though the text mentions \"greent\" or money, it doesn't necessarily have to be classified as a Sports article, as the term \"green\" in this context is indicative of profit or capital gained in financial markets.\n",
    "\n",
    "  Additionally, Sci/Tech and World classifications are unlikely, as the text doesn't contain specific information related to those domains.</s>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that while it provides the correct answer, the response is very verbose.  This a prime example of where fine-tuning can help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>_Prepare Dataset_</font>\n",
    "\n",
    "As described earlier, we can use the _AG News_ dataset on Hugging Face to fine-tune Mistral 7B.  To apply a training dataset, you need to make sure it is in __json__ format with a text column that provides the prompt and response (preferably in the model syntax). You can find `prepare_data.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Fine Tune - Prepare Dataset\n",
    "#############################\n",
    "\n",
    "# import libraries\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# create a prompt/response row\n",
    "def create_text_row(row):\n",
    "    text_row = f\"<s>[INST] Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech. {row['text']} [/INST] \\\\n {row['label']} </s>\"\n",
    "    return text_row\n",
    "\n",
    "# process the dataframe to jsonl\n",
    "def process_dataframe_to_jsonl(output_file_path, df):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for _, row in df.iterrows():\n",
    "            json_object = {\n",
    "                \"text\": create_text_row(row),\n",
    "                \"label\": row[\"label\"]\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "\n",
    "# load the dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# jsonl file paths\n",
    "train_json_file = \"/kellogg/proj/awc6034/fine_tune/ag_news/data/train_data.jsonl\"\n",
    "test_json_file = \"/kellogg/proj/awc6034/fine_tune/ag_news/data/test_data.jsonl\"\n",
    "\n",
    "# convert data to pandas\n",
    "train_df = train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "\n",
    "# process and save json files\n",
    "process_dataframe_to_jsonl(train_json_file, train_df)\n",
    "process_dataframe_to_jsonl(test_json_file, test_df)\n",
    "\n",
    "print(\"===========================\")\n",
    "print(\"Data processing complete.\")\n",
    "print(\"Here is a sample of the training data:\")\n",
    "print(test_df.head())\n",
    "print(\"===========================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Output:__  \n",
    "```\n",
    "===========================\n",
    "Data processing complete.\n",
    "Here is a sample of the training data:\n",
    "                                                text  label\n",
    "0  Christopher Lee Becoming a Video Game Wizard (...      3\n",
    "1  Law Must Respond to Internet Revolution CURREN...      3\n",
    "2  Woods give up number one to Vijay Fame is nice...      1\n",
    "3  Alcoa announces a profits warning US aluminium...      2\n",
    "4  India lauds Arafat's lifetime devotion to Pale...      0\n",
    "===========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='purple'>_Fine Tuning Steps_</font>\n",
    "\n",
    "Once the data is prepared, we can fine-tune the pre-trained Mistral model using a combination of techniques for efficiency and effectiveness.\n",
    "\n",
    "#### <font color='purple'>__Techniques__</font>\n",
    "\n",
    "* <font color='purple'>__PEFT (Parameter-Efficient Fine-Tuning)__</font>: Imagine a large toolbox filled with tools. Standard fine-tuning would pull out every tool and try to adjust them all. PEFT, however, uses \"adapter modules\" like smaller, specialized toolkits. These focus updates on the most relevant parts of the model, saving time and resources by not retraining everything.\n",
    "\n",
    "* <font color='purple'>__LoRA (Low-Rank Adaptation)__</font>: When fine-tuning, it's crucial to identify the areas of the model most important for the new task. LoRA acts like a spotlight, shining on these key areas within the vast network. PEFT then utilizes this information to target those specific parts with the adapter modules for focused updates.\n",
    "\n",
    "* <font color='purple'>__SFT (Supervised Fine-Tuning)__</font>: Fine-tuning involves adjusting the model's internal parameters. Standard training might make large, abrupt changes. SFT takes a gentler approach, making smaller, more nuanced adjustments. This allows the model to refine its understanding of the data without drastically altering its existing knowledge, potentially leading to better overall performance.\n",
    "\n",
    "#### <font color='purple'>__Fine-Tuning Steps__</font>\n",
    "\n",
    "Today we'll load a pre-trained Mistral model and prepare it for training. We then identify the most important parts in the model using LoRA. These modules are targeted in the PEFT process. The model is then fine-tuned using the SFTTrainer, which uses the SFT method to update the model parameters.\n",
    "\n",
    "After the fine-tuning process, we save the fine-tuned model. Then, we loads the base model and the fine-tuned model, and merge them together. The merged model is then saved for future use. This merging process allows the fine-tuned model to benefit from the knowledge in the base model, while also incorporating the updates made during the fine-tuning process.\n",
    "\n",
    "You can find `fine_tune.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Fine Tuning using PEFT, LoRA, and SFT\n",
    "#######################################\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import bitsandbytes as bnb\n",
    "from lora import LoraConfig, get_peft_model, PeftModel\n",
    "from sft import SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# model paths\n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/mistral_mistralAI\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# quantized model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir=llm_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True, cache_dir=llm_dir)\n",
    "\n",
    "# load the train and test datasets\n",
    "train_data = load_from_disk(\"/kellogg/proj/awc6034/fine_tune/ag_news/data/train_data.jsonl\")\n",
    "test_data = load_from_disk(\"/kellogg/proj/awc6034/fine_tune/ag_news/data/test_data.jsonl\")\n",
    "\n",
    "# find modules in the model\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "# configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# calculate the number of trainable parameters\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "\n",
    "# set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# clear GPU memory cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# setup and start training\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=lora_config,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=0.03,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save the model\n",
    "new_model = \"mistralai-Code-Instruct-ag_news\" #Name of the model you will be pushing to huggingface model hub\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "# merge the models\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# save the merged model\n",
    "merged_model.save_pretrained(new_model, safe_serialization=True)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>_Test Fine-Tuned Model_</font>\n",
    "\n",
    "Finally, we can test the fine-tuned model on our test dataset. Here we'll revisit our initial query. You can find `query_finetune.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Fine Tune - Query Merged Model\n",
    "################################\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# model path\n",
    "llm_path = \"/kellogg/proj/awc6034/fine_tune/ag_news/mistralai-Code-Instruct-ag_news\"\n",
    "\n",
    "# quantized model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model loading\n",
    "llm_path = \"/kellogg/proj/awc6034/fine_tune/ag_news/mistralai-Code-Instruct-ag_news\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, add_eos_token=True)\n",
    "\n",
    "# query function\n",
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <s>\n",
    "  [INST]\n",
    "  Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech.\n",
    "  {query}\n",
    "  [/INST]\n",
    "  </s>\n",
    "  <s>\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encodeds.to(device)\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])\n",
    "\n",
    "# query\n",
    "query = \"\"\"\n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\"\"\"\n",
    "\n",
    "# submit query\n",
    "result = get_completion(query=query, model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Output from fine tuned model\n",
    "\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.82s/it]\n",
    "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "<s> \n",
    "  <s> \n",
    "  [INST]\n",
    "  Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech.\n",
    "  \n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\n",
    "  [/INST]\n",
    "  </s> \n",
    "  <s> \n",
    "\n",
    "  </s> The text appears to fall into class 2 - Business. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output still contains slightly more text than we requested, it still provides a concise response of `class 2 - Business`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>_Reference Sources_</font>\n",
    "\n",
    "- [A Beginner's Guide to Fine-Tuning Mistral-7B Instruct Model](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)\n",
    "- [Running Fine Tuning with qlora and sft](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)\n",
    "- [Mistral Colab Finetune Notebook](https://github.com/adithya-s-k/LLM-Alchemy-Chamber/blob/main/LLMs/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final.ipynb?source=post_page-----0f39647b20fe--------------------------------)\n",
    "- [Explanation for SFT](https://huggingface.co/docs/trl/main/en/sft_trainer)\n",
    "- [Explanation for qlora, quantization, and finetuning](https://blog.paperspace.com/mistral-7b-fine-tuning/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anova_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
