

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Using GPUs at Northwestern &#8212; Kellogg Research Support Open Source LLM Cookbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'slurm_gpu_usage';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Using Transformers to run LLMs" href="transformers.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="welcome.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Kellogg Research Support Open Source LLM Cookbook - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Kellogg Research Support Open Source LLM Cookbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Open Source Large Language Models (LLMs)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Open Source LLMs on KLC</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Using GPUs at Northwestern</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Using Transformers to run LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="llama-cpp.html">Using Llama-cpp-python to run LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_case.html">Example Use Case: 10K Processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Techniques</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="retrieval.html">Retrieval Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="fine-tuning.html">Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="vision.html">Image Processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="takeaways.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/rs-kellogg/krs-openllm-cookbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rs-kellogg/krs-openllm-cookbook/issues/new?title=Issue%20on%20page%20%2Fslurm_gpu_usage.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/slurm_gpu_usage.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Using GPUs at Northwestern</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-computing-for-llms">Parallel Computing for LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-gpu-python-code">Sample GPU Python Code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slurm-script-to-access-gpu-nodes">SLURM Script to Access GPU Nodes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-sources">Reference Sources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="using-gpus-at-northwestern">
<h1>Using GPUs at Northwestern<a class="headerlink" href="#using-gpus-at-northwestern" title="Permalink to this heading">#</a></h1>
<div class="important admonition">
<p class="admonition-title">Quest allocation</p>
<p>In this workshop, we‚Äôll leverage the power of Quest GPU nodes to run our open-source LLMs. To do so, please use the temporary Quest allocation: <font color='purple'><strong>e32337</strong></font>.</p>
<p>Afterwards, you can request your own Quest allocation <a class="reference external" href="https://www.it.northwestern.edu/departments/it-services-support/research/computing/quest/general-access-allocation-types.html">here</a></p>
<figure class="align-default" id="high-performance-computing">
<a class="reference internal image-reference" href="_images/high-performance-computing.jpg"><img alt="_images/high-performance-computing.jpg" src="_images/high-performance-computing.jpg" style="width: 500px;" /></a>
</figure>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are other options for GPUs:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://colab.research.google.com/?utm_source=scs-index">Google Colab</a> allows you to use GPUs for free with browser-based notebooks</p></li>
<li><p>Cloud platforms like Amazon Web Services, Google Cloud Platform, and Microsoft Azure all offer cloud-based GPUs for a price</p></li>
<li><p>Many other cloud providers have sprung up, such as <a class="reference external" href="https://www.paperspace.com/">Paperspace</a></p></li>
<li><p>You can buy your own if you have the budget and expertise</p></li>
</ul>
</div>
<section id="parallel-computing-for-llms">
<h2>Parallel Computing for LLMs<a class="headerlink" href="#parallel-computing-for-llms" title="Permalink to this heading">#</a></h2>
<div class="important admonition">
<p class="admonition-title">LLM Acceleration</p>
<p>The purpose of running our LLMs on GPU nodes is to speed up processing.  In order to understand this, you‚Äôll often hear us talk about <font color='purple'><strong>CPUs</strong></font>, <font color='purple'><strong>GPUs</strong></font>, and <font color='purple'><strong>CUDA</strong></font>.  This section breaks down these terms.</p>
</div>
<div class="admonition-cpu admonition">
<p class="admonition-title">CPU</p>
<p>Much like your own computer, some of our KLC and Quest nodes are equipped with both processors and graphics cards. A processor or <font color='purple'><strong>central processing unit (CPU)</strong></font> is responsible for all the mathematical and logical calculations on a node. In a nutshell, it runs code. While CPUs are extremely powerful and complete most tasks in an infinitesimally short amount of time, a CPU core can only handle one task at a time and runs things <strong>sequentially</strong>.</p>
<figure class="align-default" id="cpu-sequential">
<a class="reference internal image-reference" href="_images/cpu_sequential.png"><img alt="_images/cpu_sequential.png" src="_images/cpu_sequential.png" style="width: 500px;" /></a>
</figure>
</div>
<div class="admonition-multiple-cpu-cores admonition">
<p class="admonition-title">Multiple CPU Cores</p>
<p>One way to speed up processing is through <font color='purple'><em>parallel computing</em></font> across multiple CPU cores. Parallel computing is a method of solving a single problem by breaking it down into smaller chunks that run <strong>simultaneously</strong>.  A CPU can break up a task and distributes it over multiple CPU cores.</p>
<figure class="align-default" id="cpu-parallel">
<a class="reference internal image-reference" href="_images/cpu_parallel.png"><img alt="_images/cpu_parallel.png" src="_images/cpu_parallel.png" style="width: 350px;" /></a>
</figure>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The latest generation of <a class="reference external" href="https://www.kellogg.northwestern.edu/academics-research/research-support/computing/kellogg-linux-cluster.aspx">KLC nodes</a> have 64 CPU cores and 2TB of shared RAM üöÄ. This means you could in theory run 64 parallel (simultaneous) processes on a single KLC node.</p>
</div>
<div class="admonition-gpus admonition">
<p class="admonition-title">GPUs</p>
<p>A graphics card or <font color='purple'><strong>graphics processing unit (GPU)</strong></font> is a specialized hardware component that can efficiently handle parallel mathematical operations. In comparison to the 24 cores you can use on KLC, a A100 GPU contains 6,912 CUDA cores (the H100 GPU has an astounding 18,432 CUDA cores).  While a GPU core is less powerful than an individual CPU core, their sheer volume make them ideal for handling certain kinds of large amounts of computations in parallel, especially the vector and matrix operations for which GPUs were designed. We will see an example later of the speedup that GPUs provide for this kind of task.</p>
<figure class="align-default" id="gpu">
<a class="reference internal image-reference" href="_images/gpu.png"><img alt="_images/gpu.png" src="_images/gpu.png" style="width: 350px;" /></a>
</figure>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If GPUs are so much better at parallelization than CPUs, why aren‚Äôt all tasks given to GPUs?</p>
<ul class="simple">
<li><p>Some tasks simply can‚Äôt be parallelized, if the input to one depends on the output from another. In this case, they must be run in serial for logical reasons.</p></li>
<li><p>Even when parallelization is possible, some tasks actually take longer if parallelized. Sometimes the overhead of coordinating processes across cores might actually take longer than having a single CPU core complete the task alone.</p></li>
</ul>
</div>
<div class="admonition-cuda admonition">
<p class="admonition-title">CUDA</p>
<p>The potential inefficiency of parallelization raises the question of how your system knows when to send a task to CPUs or to GPUs? For Nvidia-based GPU‚Äôs, this is where <font color='purple'><strong>CUDA</strong></font> comes in.  <font color='purple'><strong>CUDA (Compute Unified Device Architecture)</strong></font> is a powerful software platform that helps computer programs run faster. On the GPU nodes, we use it to solve performance intensive problems by optimizing when to allocate certains tasks to CPU processing or GPU processing.</p>
<p>In this animation, CUDA determines which tasks to delegate to GPUs or to CPUs.</p>
<p><img alt="cuda" src="_images/giffy_gif.gif" /></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will not typically directy program in CUDA, nor most of you in Pytorch/Tensorflow. Most of you will probably stick to using the highest layers of abstraction, such as the Hugging Face <a class="reference external" href="https://huggingface.co/docs/transformers/index">Transformer</a> library. However, it is sometimes necessary to know which version of CUDA or Pytorch/Tensorflow you need to have installed.</p>
</div>
</section>
<section id="sample-gpu-python-code">
<h2>Sample GPU Python Code<a class="headerlink" href="#sample-gpu-python-code" title="Permalink to this heading">#</a></h2>
<div class="important admonition">
<p class="admonition-title">Testing for GPU availability</p>
<p>To get started with the GPU nodes, here is a sample Python script. The code below allows you to test whether GPUs are available on a node and runs tensors. This file is located in the course <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/slurm_basics">github repository</a></p>
</div>
<div class="admonition-pytorch-gpu-test-py admonition">
<p class="admonition-title">pytorch_gpu_test.py</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Check if CUDA is available, and which version</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA version </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="si">}</span><span class="s2"> is available&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of GPUs available:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CUDA is not available.&quot;</span><span class="p">)</span>

<span class="c1"># Check if CUDA is available and set the device accordingly</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Print whether a GPU or CPU is being used</span>
<span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using GPU&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using CPU&quot;</span><span class="p">)</span>

<span class="c1"># Create two random tensors</span>
<span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Add the two tensors, the operation will be performed on the GPU if available</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tensor1</span> <span class="o">+</span> <span class="n">tensor2</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Take note!</p>
<p>For vector and matrix operations, GPUs is orders of magnitude faster than CPUs</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Code execution in a Jupyter notebook is demonstrated in <a class="reference external" href="https://kellogg-shared.s3.us-east-2.amazonaws.com/videos/quest-on-demand-gpu-notebook.mp4">this video</a></p>
</div>
</section>
<section id="slurm-script-to-access-gpu-nodes">
<h2>SLURM Script to Access GPU Nodes<a class="headerlink" href="#slurm-script-to-access-gpu-nodes" title="Permalink to this heading">#</a></h2>
<div class="important admonition">
<p class="admonition-title">Slurm scripts</p>
<p>For this workshop, we‚Äôll submit jobs to the Quest GPU nodes through a <font color='purple'>SLURM</font> (scheduler) script. You can launch the sample python code using this script.</p>
<p>Documentation on using <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964">slurm on Quest</a></p>
</div>
<div class="admonition-font-color-purple-northwestern-font-gpu-resources admonition">
<p class="admonition-title"><font color='purple'><em>Northwestern</em></font> GPU Resources</p>
<p><a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1112">Quest</a> has dozens of Nvidia-based GPU nodes available for use. We will show you how to access them via a Jupyter notebook using <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=2234">Quest on Demand</a> and using the <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1964">Slurm scheduler</a>. Both of these methods require that you are part of a Quest allocation.</p>
</div>
<!-- :::{admonition} _For the Kellogg Community Only_


We are in the process of setting up GPU nodes for exclusive use by the Kellogg research community as part of the [Kellogg Linux Cluster](https://www.kellogg.northwestern.edu/academics-research/research-support/computing/kellogg-linux-cluster.aspx). Accessing these nodes will be identical to accessing GPU nodes on Quest, but will require only a KLC account and not a separate Quest allocation.

```{figure} ./images/KLC-announcement.png
---
width: auto
name: KLC-announcment
---
::: --><div class="admonition-font-color-purple-pytorch-gpu-test-sh-font admonition">
<p class="admonition-title"><font color='purple'><em>pytorch_gpu_test.sh</em></font></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --account=e32337</span>
<span class="c1">#SBATCH --partition gengpu</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --gres=gpu:a100:1</span>
<span class="c1">#SBATCH --constraint=pcie</span>
<span class="c1">#SBATCH --time 0:30:00</span>
<span class="c1">#SBATCH --mem=40G</span>
<span class="c1">#SBATCH --output=/projects/e32337/slurm-output/slurm-%j.out</span>


<span class="n">module</span> <span class="n">purge</span> <span class="nb">all</span>
<span class="n">module</span> <span class="n">use</span> <span class="o">--</span><span class="n">append</span> <span class="o">/</span><span class="n">kellogg</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">Modules</span><span class="o">/</span><span class="n">modulefiles</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">micromamba</span><span class="o">/</span><span class="n">latest</span>
<span class="n">source</span> <span class="o">/</span><span class="n">kellogg</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">Modules</span><span class="o">/</span><span class="n">modulefiles</span><span class="o">/</span><span class="n">micromamba</span><span class="o">/</span><span class="n">load_hook</span><span class="o">.</span><span class="n">sh</span>
<span class="n">micromamba</span> <span class="n">activate</span> <span class="o">/</span><span class="n">kellogg</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">llm</span><span class="o">-</span><span class="n">test</span><span class="o">-</span><span class="n">env</span>
<span class="n">python</span> <span class="n">pytorch_gpu_test</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
<div class="admonition-breaking-down-this-script admonition">
<p class="admonition-title">Breaking down this script</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--account</span></code> is the <a class="reference external" href="https://www.it.northwestern.edu/departments/it-services-support/research/computing/quest/general-access-allocation-types.html">Quest allocation</a> you are given.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--partition=gengpu</span></code> directs you to <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1112">GPU nodes</a> on the Quest Genomics Cluster</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--ntasks-per-node=1</span></code> this line specifies how many cores of the node you will use. Setting <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node=2</span></code> will run your script on two cores of the node. Only adjust this parameter if your code is parallelizable, otherwise it will slow your job down, not speed it up.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:a100:1</span></code> This line specifies that the job requires 1 GPU of type ‚Äúa100‚Äù. You can select more.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--constraint</span></code> Specifies the type of A100 preferred, <a class="reference external" href="https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1112">choices</a> are ‚Äúsxm‚Äù (80GB of GPU memory) or ‚Äúpcie‚Äù (40GB of GPU memory)- <code class="docutils literal notranslate"><span class="pre">--nodes=1</span></code> specifies that the job will be run on 1 node of the cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--time==00:30:00</span></code> indicates that this job will be allowed to run for up to 30 minutes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mem</span></code> specifies how much memory you are requesting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--output</span></code> specifies the path and file where the stdout and stderr output streams will get saved.</p></li>
</ul>
<p>After accessing the GPU node, the script loads python and activates the <font color='purple'>llm-test-env</font> conda environmen, which has all the necessary python packages installed. Finally it executes the python code.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Demonstration of executing a slurm script using Quest On Demand graphical interface is shown <a class="reference external" href="https://kellogg-shared.s3.us-east-2.amazonaws.com/videos/quest-on-demand-gpu-slurm.mp4">here</a>, and using a command line terminal <a class="reference external" href="https://kellogg-shared.s3.us-east-2.amazonaws.com/videos/console-gpu-slurm.mp4">here</a>.</p>
</div>
</section>
<section id="reference-sources">
<h2>Reference Sources<a class="headerlink" href="#reference-sources" title="Permalink to this heading">#</a></h2>
<div class="important admonition">
<p class="admonition-title">Links</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtube.com/watch?v=r9IqwpMR9TE">Cuda Simply Explained</a></p></li>
<li><p><a class="reference external" href="https://blog.paperspace.com/demystifying-parallel-computing-gpu-vs-cpu-explained-simply-with-cuda/">Understanding Parallel Computing</a></p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="transformers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Using Transformers to run LLMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-computing-for-llms">Parallel Computing for LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-gpu-python-code">Sample GPU Python Code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slurm-script-to-access-gpu-nodes">SLURM Script to Access GPU Nodes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-sources">Reference Sources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kellogg Research Support
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      ¬© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>