{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "<font color='purple'>__Fine-tuning__</font> is a supervised learning process that uses a data set of labeled examples to update the weights of an LLM. The labeled examples or prompt-completion pairs are used to improve the LLM's ability to generate good completions for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![finetune](./images/finetuning.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>__Fine-tuning__</font> takes LLMs beyond basic prompting by allowing you to train a model on a larger dataset specifically tailored to your desired task, leading to several advantages:\n",
    "\n",
    "* ___Superior Results___: Compared to prompting, fine-tuning delivers higher quality outputs because the model learns directly from a wider range of examples.\n",
    "\n",
    "* ___Efficiency___: You can train on more data than can fit in a single prompt, saving on token usage and reducing latency for requests.\n",
    "\n",
    "* ___Specificity___: Fine-tuning excels at tasks requiring specific styles, tones, formats, or consistent outputs. It enhances reliability in achieving your desired results, handles complex instructions better, and tackles edge cases effectively.\n",
    "\n",
    "* ___New Skills___: Fine-tuning empowers the model to master new skills or complete tasks that are difficult to articulate through simple prompts.\n",
    "\n",
    "### __What you need__\n",
    "\n",
    "<font color='purple'>__Fine-tuning__</font> is a __time-intensive__ process that requires:\n",
    "\n",
    "__1.) <font color='purple'>a ground-truth dataset__</font> -  This is a comprehensive and high-quality dataset that the model can learn from. It should be representative of the problem space and contain examples of the types of tasks you want the model to perform.\n",
    "\n",
    "__2.) <font color='purple'>intricate code__</font> - Fine-tuning a model is not a straightforward process. It requires intricate, carefully crafted code that can effectively guide the model's learning process. This includes setting up the learning rate, batch size, number of epochs, and other hyperparameters, as well as implementing the training loop. \n",
    "\n",
    "__3.) <font color='purple'>iteration__</font> - Fine-tuning an LLM is rarely a \"one and done\" task. It often involves multiple iterations of training, evaluation, and refinement. After each round of training, the model's performance should be evaluated using a separate validation dataset. Based on the results, adjustments may need to be made to the model architecture, hyperparameters, or training data. This iterative process continues until the model's performance meets the desired criteria. \n",
    "\n",
    "\n",
    "### __When to use it__\n",
    "\n",
    "Before delving into the mechanics of fine-tuning; note that it might not always be the best solution.  Here's a breakdown of when to apply prompt-engineering, RAG, or fine-tuning:\n",
    "\n",
    "<font color='purple'>_Use Prompt Engineering for_</font>:\n",
    "\n",
    "* Achieving good results for simpler tasks without extensive training data. You can improve prompts with: \n",
    "    * <font color='purple'>__Few-shot in-context learning__</font> - providing examples of respsonses within the prompt\n",
    "    * <font color='purple'>__Prompt chaining__</font> - breaking down complex tasks into smaller, sequential prompts for the model to follow.\n",
    "\n",
    "<font color='purple'>_Use Retrieval-Augmented Generation (RAG) for_</font>:\n",
    "\n",
    "* Situations with a large, relevant document database.\n",
    "* Tasks requiring factual accuracy where context retrieval is beneficial.\n",
    "\n",
    "<font color='purple'>_Use Fine-Tuning for_</font>:\n",
    "\n",
    "* Tasks requiring consistent outputs in a particular style, tone, or format.\n",
    "* Achieving specific behaviors from the model.\n",
    "* Mastering complex tasks that are difficult to express through simple prompts.\n",
    "\n",
    ":::{note}\n",
    "Our advice is to experiment with crafting effective instructions and prompts for the model before attempting fine-tuning. Prompt engineering is low-cost and does not require any programming. Different prompting techniques can be used, such as few-shot in-context learning and chain-of-thought prompts. Fine-tuning requires more effort -- it requires a collection of supervised training examples, some programming work, extra GPU resources, and extra storage for a model with updated weights.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Step 1 - Test base models__\n",
    "\n",
    "In this workshop, we'll demonstrate how to fine-tune a Mistral 7B model. Mistral is especially apt for performing classification tasks. \n",
    "\n",
    "The first step in fine-tuning should always be to properly load and test out-of-the-box models.  We recommend trying a few different models and model sizes. The code below loads a quantized Mistral model using Transformers.  It uses the first text example from the _AG News_ dataset on Hugging Face here: https://huggingface.co/datasets/ag_news.  It asks the model to classify this text into 1 of 4 categories:\n",
    "\n",
    "* 0 - World News\n",
    "* 1 - Sports\n",
    "* 2 - Business\n",
    "* 3 - Science and Technology.\n",
    "\n",
    "It places this prompt in the Mistral __prompt syntax__.  You can find `query_initial.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Fine Tune - Initial Query\n",
    "#############################\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# model paths\n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/mistral_mistralAI\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# quantized model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir=llm_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True, cache_dir=llm_dir)\n",
    "\n",
    "# query function\n",
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <s>\n",
    "  [INST]\n",
    "  Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech.\n",
    "  {query}\n",
    "  [/INST]\n",
    "  </s>\n",
    "  <s>\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encodeds.to(device)\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])\n",
    "\n",
    "# query\n",
    "query = \"\"\"\n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\"\"\"\n",
    "\n",
    "# submit query\n",
    "result = get_completion(query=query, model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output:__  \n",
    "```\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.30s/it]\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.32s/it]\n",
    "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "<s> \n",
    "  <s> \n",
    "  [INST]\n",
    "  Below is a text from AG News dataset. Classify it into one of the four classes: World, Sports, Business, Sci/Tech.\n",
    "  \n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\n",
    "  [/INST]\n",
    "  </s> \n",
    "  <s> \n",
    "\n",
    "  </s> This text can be classified as: Business.\n",
    "\n",
    "  The reason for this classification is that the text discusses \"Wall Street's dwindling band of ultra-cynics\" and \"short-sellers\" who are \"seeing green again.\" These terms are typically associated with finance and investing, which falls within the Business category. The mention of Wall Street is also a strong indicator of a Business article.\n",
    "\n",
    "  Please note that even though the text mentions \"greent\" or money, it doesn't necessarily have to be classified as a Sports article, as the term \"green\" in this context is indicative of profit or capital gained in financial markets.\n",
    "\n",
    "  Additionally, Sci/Tech and World classifications are unlikely, as the text doesn't contain specific information related to those domains.</s>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that while it provides the correct answer, the response is very verbose.  This a prime example of where fine-tuning can help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Step 2 - Prepare Dataset__\n",
    "\n",
    "As described earlier, we can use the _AG News_ dataset on __Hugging Face__ to fine-tune Mistral 7B.  To apply a training dataset, you need to make sure it is in __json__ format with a column that provides the prompt and response (preferably in the model syntax). You can find `prepare_data.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Fine Tune - Prepare Dataset\n",
    "#############################\n",
    "\n",
    "# import libraries\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# create a prompt/response row\n",
    "def create_text_row(row):\n",
    "    text_row = f\"<s>[INST] Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech. {row['text']} [/INST] \\\\n {row['label']} </s>\"\n",
    "    return text_row\n",
    "\n",
    "# process the dataframe to jsonl\n",
    "def process_dataframe_to_jsonl(output_file_path, df):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for _, row in df.iterrows():\n",
    "            json_object = {\n",
    "                \"text\": create_text_row(row),\n",
    "                \"label\": row[\"label\"]\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "\n",
    "# load the dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# jsonl file paths\n",
    "train_json_file = \"/kellogg/data/llm_models_opensource/mistral_mistralAI/fine_tune/ag_news/data/train_data.jsonl\"\n",
    "test_json_file = \"/kellogg/data/llm_models_opensource/mistral_mistralAI/fine_tune/ag_news/data/test_data.jsonl\"\n",
    "\n",
    "# convert data to pandas\n",
    "train_df = train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "\n",
    "# process and save json files\n",
    "process_dataframe_to_jsonl(train_json_file, train_df)\n",
    "process_dataframe_to_jsonl(test_json_file, test_df)\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "test_df = pd.read_json(test_json_file, lines=True)\n",
    "\n",
    "print(\"===========================\")\n",
    "print(\"Data processing complete.\")\n",
    "print(\"Here is a sample of the training data:\")\n",
    "print(test_df.head())\n",
    "print(\"test_df text value:\" + test_df['text'][0])\n",
    "print(\"===========================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Output:__  \n",
    "```\n",
    "===========================\n",
    "Data processing complete.\n",
    "Here is a sample of the training data:\n",
    "                                                text  label\n",
    "0  <s>[INST] Classify the following text into one...      3\n",
    "1  <s>[INST] Classify the following text into one...      1\n",
    "2  <s>[INST] Classify the following text into one...      2\n",
    "3  <s>[INST] Classify the following text into one...      0\n",
    "4  <s>[INST] Classify the following text into one...      1\n",
    "===========================\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### __Step 3 - Fine Tune__\n",
    "\n",
    "Once the data is prepared, we can fine-tune the pre-trained Mistral model using a combination of techniques for efficiency and effectiveness.\n",
    "\n",
    "#### <font color='purple'>_Techniques_</font>\n",
    "\n",
    "* <font color='purple'>__Full Fine-Tuning__</font>: involves adjusting __all the parameters__ of the LLM during the training process. This approach allows the model to fully adapt to the new task, potentially leading to higher performance. However, it requires a large amount of data and computational resources, and there's a risk of __catastrophic forgetting__ (the model could forget to do tasks it could before.)\n",
    "\n",
    "* <font color='purple'>__Partially Embedded Fine-Tuning (PEFT)__</font>, on the other hand, __only fine-tunes a subset of the model's parameters__. This strategy requires less memory and computational resources to perform, making it a more efficient choice in certain scenarios. By only updating a portion of the model, it may not be as effective when the model needs to significantly alter its initial learning or \"forget\" tasks it initially performed poorly. Two of the most effective PEFT methods are:\n",
    "\n",
    "    * __Reparameterization__ - reparameterizes model weights into a low-rank format. One example is <font color='purple'>__LoRA (Low-Rank Adaptation)__</font>, which uses rank decomposition matrices to efficiently update model parameters.\n",
    "\n",
    "    * __Additive__ - freezes all the original LLM weights while adding trainable layers or paramters to the model. One example is called __prompt tuning__ that freezes model weights, but adds trainable tokens (__soft prompts__) to the prompt embeddings.  This only updates the prompt weights. \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lora](./images/lora.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    "\n",
    "#### <font color='purple'>_Steps_</font>\n",
    "\n",
    "We'll load a pre-trained Mistral model and prepare it for training. We then identify the most important parts in the model using LoRA. These modules are targeted in the PEFT process. The model is then fine-tuned using the SFTTrainer, which uses the SFT method to update the model parameters.\n",
    "\n",
    "After the fine-tuning process, we save the fine-tuned model. Then, we loads the base model and the fine-tuned model, and merge them together. The merged model is then saved for future use. This merging process allows the fine-tuned model to benefit from the knowledge in the base model, while also incorporating the updates made during the fine-tuning process.\n",
    "\n",
    "You can find `fine_tune.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Fine Tuning using PEFT, LoRA, and SFT\n",
    "#######################################\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import bitsandbytes as bnb\n",
    "from lora import LoraConfig, get_peft_model, PeftModel\n",
    "from sft import SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# model paths\n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/mistral_mistralAI\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# quantized model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir=llm_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True, cache_dir=llm_dir)\n",
    "\n",
    "# load the train and test datasets\n",
    "train_data = load_from_disk(\"/kellogg/data/llm_models_opensource/mistral_mistralAI/fine_tune/ag_news/data/train_data.jsonl\")\n",
    "test_data = load_from_disk(\"/kellogg/data/llm_models_opensource/mistral_mistralAI/fine_tune/ag_news/data/test_data.jsonl\")\n",
    "\n",
    "# find modules in the model\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "# configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# calculate the number of trainable parameters\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "\n",
    "# set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# clear GPU memory cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# setup and start training\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=lora_config,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=0.03,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save the model\n",
    "new_model = \"mistralai-Code-Instruct-ag_news\" #Name of the model you will be pushing to huggingface model hub\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "# merge the models\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# save the merged model\n",
    "merged_model.save_pretrained(new_model, safe_serialization=True)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Test Fine-tuned Model\n",
    "\n",
    "Finally, we can test the fine-tuned model on our test dataset. Here we'll revisit our initial query. You can find `query_finetune.py` script in the [scripts/fine_tune](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/fine_tune) folder of our github repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Fine Tune - Query Merged Model\n",
    "################################\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "\n",
    "# quantized model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model path\n",
    "llm_path = \"/kellogg/data/llm_models_opensource/mistral_mistralAI/fine_tune/ag_news/mistralai-Code-Instruct-ag_news\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_path, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, add_eos_token=True)\n",
    "\n",
    "# query function\n",
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <s>\n",
    "  [INST]\n",
    "  Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech.\n",
    "  {query}\n",
    "  [/INST]\n",
    "  </s>\n",
    "  <s>\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encodeds.to(device)\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])\n",
    "\n",
    "# query\n",
    "query = \"\"\"\n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\"\"\"\n",
    "\n",
    "# submit query\n",
    "result = get_completion(query=query, model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output:__  \n",
    "```\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "/kellogg/software/envs/gpu-llama2/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
    "  torch.utils._pytree._register_pytree_node(\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.82s/it]\n",
    "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "<s> \n",
    "  <s> \n",
    "  [INST]\n",
    "  Classify the following text into one of the four classes: 0 - World, 1 - Sports, 2 - Business, 3 - Sci/Tech.\n",
    "  \n",
    "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band \n",
    "of ultra-cynics, are seeing green again.\n",
    "\n",
    "  [/INST]\n",
    "  </s> \n",
    "  <s> \n",
    "\n",
    "  </s> The text appears to fall into class 2 - Business. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output still contains slightly more text than we requested, it still provides a concise response of `class 2 - Business`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Reference Sources__\n",
    "\n",
    "- [A Beginner's Guide to Fine-Tuning Mistral-7B Instruct Model](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)\n",
    "- [Running Fine Tuning with qlora and sft](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)\n",
    "- [Mistral Colab Finetune Notebook](https://github.com/adithya-s-k/LLM-Alchemy-Chamber/blob/main/LLMs/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final.ipynb?source=post_page-----0f39647b20fe--------------------------------)\n",
    "- [Explanation for SFT](https://huggingface.co/docs/trl/main/en/sft_trainer)\n",
    "- [Explanation for qlora, quantization, and finetuning](https://blog.paperspace.com/mistral-7b-fine-tuning/)\n",
    "- [Low Rank Adaptation: A technical deep dive](https://www.ml6.eu/blogpost/low-rank-adaptation-a-technical-deep-dive)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anova_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
