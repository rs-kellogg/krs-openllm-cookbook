{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Transformers to run LLMs  \n",
    "\n",
    "#### <font color='purple'>Basic workflow for using transformers</font>\n",
    "1. Load the model parameters\n",
    "2. Convert prompt query to tokens\n",
    "3. Call model to process tokens and generate response tokens\n",
    "4. Decode tokens to text response\n",
    "\n",
    "![transformers](./images/transformers.png)  \n",
    "\n",
    "_Adapted from teaching materials created by DeepLearning.AI._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Run Llama2 model with transformers</font>  \n",
    "Llama2 model on the Hugging Face site: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf  \n",
    "##### Python script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# transformers calling Llama2 model\n",
    "##################################\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def run_llama2(llm_dir, llm_model, query, customize_setting):\n",
    "\n",
    "    #######################################\n",
    "    # 1. Load the model parameters\n",
    "    #######################################\n",
    "    # The quantization_config is optional; use it for very large model; it reduces memory and computational costs by representing weights and activations with lower-precision data types\n",
    "    # To use quantization, uncomment the following two lines and comment out the current \"model = \" line\n",
    "    # quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(llm_model,cache_dir=llm_dir, device_map=\"auto\", quantization_config=quantization_config)\n",
    "    model = AutoModelForCausalLM.from_pretrained(llm_model,cache_dir=llm_dir, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model, cache_dir=llm_dir)\n",
    "\n",
    "    #######################################\n",
    "    # 2. Convert prompt query to tokens\n",
    "    #######################################\n",
    "    device = \"cuda\"\n",
    "    model_input = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(f\"=== Customized setting:\")\n",
    "    for key, value in customize_setting.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    #######################################\n",
    "    # 3. Call model to process tokens and generate response tokens\n",
    "    #######################################\n",
    "    outputs = model.generate(**model_input, **customize_setting)\n",
    "\n",
    "    #######################################\n",
    "    # 4. Decode tokens to text response\n",
    "    #######################################\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"====================\")\n",
    "    print(f\"LLM model: {llm_model}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Response: \")\n",
    "    print(decoded)\n",
    "    print(\"====================\")\n",
    "\n",
    "    #######################################\n",
    "    # Logging\n",
    "    #######################################\n",
    "    finished_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    columns = [\"llm_model\", \"query\", \"response\", \"finished_time\"]\n",
    "    row = [llm_model, query, decoded, finished_time]\n",
    "    for key, value in customize_setting.items():\n",
    "        columns.append(key)\n",
    "        row.append(value)\n",
    "    df = pd.DataFrame([row], columns=columns)\n",
    "    llm_name = llm_model.split(\"/\")[-1]\n",
    "    log_file = Path(f\"./log_{llm_name}.csv\")\n",
    "    df.to_csv(log_file, index=False, mode='a', header=not log_file.exists())\n",
    "\n",
    "\n",
    "# Set up model directory info; set to your own project space if using new model \n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/llama2_meta_huggingface\"\n",
    "# Model name from Huggingface site\n",
    "llm_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# llm_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# llm_model = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "# For Llama2 chat, need to enclosed your prompt by [INST] and [/INST]\n",
    "query = \"[INST] Tell a fun fact about Kellogg Business School. [/INST]\"\n",
    "\n",
    "# Settings for LLM model  \n",
    "customize_setting = {\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "run_llama2(llm_dir, llm_model, query, customize_setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slurm script\n",
    "```\n",
    "# Run on Quest\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -A e32337\n",
    "#SBATCH -p gengpu\n",
    "#SBATCH --gres=gpu:a100:1\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 1\n",
    "#SBATCH -t 0:30:00\n",
    "#SBATCH --mem=40G\n",
    "\n",
    "module purge\n",
    "module load mamba/23.1.0\n",
    "source /hpc/software/mamba/23.1.0/etc/profile.d/conda.sh\n",
    "source activate /kellogg/software/envs/gpu-llama2\n",
    "\n",
    "python test_llama2.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:  \n",
    "```\n",
    "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
    "Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.81s/it]\n",
    "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.48s/it]\n",
    "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.98s/it]\n",
    "=== Loading time: 16.65569519996643 seconds\n",
    "=== Customized setting:\n",
    "    max_new_tokens: 400\n",
    "    do_sample: True\n",
    "    temperature: 0.8\n",
    "====================\n",
    "LLM model: meta-llama/Llama-2-7b-chat-hf\n",
    "Query: [INST] Tell a fun fact about Kellogg Business School. [/INST]\n",
    "Response: \n",
    "[INST] Tell a fun fact about Kellogg Business School. [/INST]  Here's a fun fact about the Kellogg School of Management at Northwestern University:\n",
    "\n",
    "Did you know that the Kellogg School of Management has its own on-campus ice cream shop? Yes, you read that right! The Kellogg Ice Cream Shop is a popular spot on campus where students can indulge in their sweet tooth and enjoy a variety of ice cream flavors, including some unique and creative flavors like \"Chocolate Chip Cookie Dough\" and \"Cinnamon Swirl.\" The shop is run by students in the Kellogg School of Management's Food, Self, and Society course, which focuses on the business of food and the social and cultural impact of food systems. So, next time you're at Northwestern University, be sure to stop by the Kellogg Ice Cream Shop and enjoy a sweet treat while learning about the business of ice cream!\n",
    "====================\n",
    "Execution time: 23.751539945602417 seconds\n",
    "Finished at: 2024-04-03 11:05:05\n",
    "====================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Run Mistral model with transformers</font>  \n",
    "Mistral model on the Hugging Face site: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2    \n",
    "Check out [scripts/transformers/test_mistral.py](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/transformers).  \n",
    "\n",
    "#### <font color='purple'>Run Gemma model with transformers</font>  \n",
    "Gemma model on the Hugging Face site: https://huggingface.co/google/gemma-7b-it  \n",
    "Check out [scripts/transformers/test_gemma.py](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/transformers).  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}