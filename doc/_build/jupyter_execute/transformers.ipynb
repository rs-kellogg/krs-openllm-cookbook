{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>__Using Transformers to run LLMs__</font>\n",
    "\n",
    "#### <font color='purple'>Basic workflow for using transformers</font>\n",
    "1. Load the model parameters\n",
    "2. Convert prompt query to tokens\n",
    "3. Call model to process tokens and generate response tokens\n",
    "4. Decode tokens to text response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Run Llama2 model with transformers</font>  \n",
    "Llama2 model on the Hugging Face site: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf  \n",
    "##### Python script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# transformers using Llama2 model\n",
    "##################################\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up model and directory info_  \n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/llama2_meta_huggingface\"\n",
    "llm_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# llm_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# llm_model = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "# 1. Load the model parameters\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model,cache_dir=llm_dir, device_map=\"auto\", quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model, cache_dir=llm_dir)\n",
    "\n",
    "# For Llama2 chat, need to enclosed your prompt by [INST] and [/INST]\n",
    "query = \"[INST] Tell a fun fact about Kellogg Business School. [/INST]\"\n",
    "\n",
    "# 2. Convert prompt query to tokens\n",
    "device = \"cuda\"\n",
    "model_input = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Settings for LLM model  \n",
    "customize_setting = {\n",
    "    \"max_new_tokens\": 400,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "print(f\"=== Customized setting:\")\n",
    "for key, value in customize_setting.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# 3. Call model to process tokens and generate response tokens\n",
    "outputs = model.generate(**model_input, **customize_setting)\n",
    "\n",
    "# 4. Decode tokens to text response\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"====================\")\n",
    "print(f\"LLM model: {llm_model}\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Response: \")\n",
    "print(decoded)\n",
    "print(\"====================\")\n",
    "\n",
    "# Logging\n",
    "finished_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "columns = [\"llm_model\", \"query\", \"response\", \"finished_time\"]\n",
    "row = [llm_model, query, decoded, finished_time]\n",
    "for key, value in customize_setting.items():\n",
    "    columns.append(key)\n",
    "    row.append(value)\n",
    "df = pd.DataFrame([row], columns=columns)\n",
    "llm_name = llm_model.split(\"/\")[-1]\n",
    "log_file = Path(f\"./log_{llm_name}.csv\")\n",
    "df.to_csv(log_file, index=False, mode='a', header=not log_file.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slurm script\n",
    "The following script is for running the GPU job on Quest. To run with Kellogg GPU, change the \"#SBATCH -A\" and \"#SBATCH -p\" lines to  \n",
    "```  \n",
    "#SBATCH -A kellogg\n",
    "#SBATCH -p kellogg\n",
    "```  \n",
    "__# run_batch_llama2.sh__  \n",
    "```\n",
    "# Run on Quest\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -A your_quest_allocation_account\n",
    "#SBATCH -p gengpu\n",
    "#SBATCH --gres=gpu:a100:1\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 1\n",
    "#SBATCH -t 0:30:00\n",
    "#SBATCH --mem=40G\n",
    "\n",
    "module purge\n",
    "module load mamba/23.1.0\n",
    "source /hpc/software/mamba/23.1.0/etc/profile.d/conda.sh\n",
    "source activate /kellogg/software/envs/gpu-llama2\n",
    "\n",
    "python test_llama2.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Run Mistral model with transformers</font>  \n",
    "Check out scripts/transformers/test_mistral.py.  \n",
    "\n",
    "#### <font color='purple'>Run Gemma model with transformers</font>  \n",
    "Check out scripts/transformers/test_gemma.py.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}