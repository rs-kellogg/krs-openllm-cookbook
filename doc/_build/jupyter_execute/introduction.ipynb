{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Objective\n",
    ":class: important\n",
    "The goals of this workshop are to look at best practices with regard to:\n",
    "\n",
    "- <font color='purple'>**Selecting and Executing**</font> open source LLMs on Quest and on the Kellogg Linux Cluster (KLC)\n",
    "- <font color='purple'>**Adapting**</font> models by using fine-tuning to improve performance and accuracy\n",
    "- <font color='purple'>**Integrating**</font> with external resources at run-time to improve LLM knowledge and reduce hallucinations\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Project Lifecycle\n",
    "\n",
    "Every LLM project goes through at least some version of this lifecycle:\n",
    "\n",
    "```{figure} ./images/project-lifecycle-1.png\n",
    "---\n",
    "width: 900px\n",
    "name: project-lifecycle-1\n",
    "---\n",
    "```\n",
    "(Diagram taken from [DeepLearning.AI](https://www.deeplearning.ai/), provided under the Creative Commons License)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Use Case\n",
    "\n",
    ":::{card}\n",
    "\n",
    "One key to success is coming up with a well-defined use case that your LLM application will implement:\n",
    "\n",
    "```{figure} ./images/project-lifecycle-2.png\n",
    "---\n",
    "width: 900px\n",
    "name: project-lifecycle-2\n",
    "---\n",
    "```\n",
    "\n",
    "Your plan should specify:\n",
    "* What data will I be using to achieve my research goal?\n",
    "* How much data do I need?\n",
    "* How will I evaluate LLM output? \n",
    "* What counts as good enough?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Types of Use Cases\n",
    "\n",
    "LLMs support different [types](https://txt.cohere.com/llm-use-cases) of use cases, often with somewhat different underlying model architectures: \n",
    "\n",
    "```{figure} ./images/LLM-use-cases.png\n",
    "---\n",
    "width: 900px\n",
    "name: LLM-use-cases\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Model\n",
    "\n",
    ":::{card} \n",
    "\n",
    "There are many open source model choices available. Why choose open source over closed source models like GPT-4?\n",
    "\n",
    "- __Reproducibility__\n",
    "- __Data privacy__\n",
    "- __Flexibility to adapt a model__\n",
    "- __Flexibility to incorporate external resources__\n",
    "- __Cost at inference time__\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Leaderboards\n",
    "\n",
    "We can use leaderboards to choose the best one for our use case, and model hubs to download and run them locally.\n",
    "\n",
    "```{figure} ./images/project-lifecycle-3-annotated.png\n",
    "---\n",
    "width: 900px\n",
    "name: project-lifecycle-3\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Models vs. Code\n",
    "\n",
    "\n",
    "```{figure} ./images/model-v-code.drawio.png\n",
    "---\n",
    "width: 900px\n",
    "name: model-v-code\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Model Hubs\n",
    "\n",
    "One widely used model hub is from [Hugging Face](https://huggingface.co/docs/hub/en/models-the-hub):\n",
    "\n",
    "```{figure} ./images/model-hub.png\n",
    "---\n",
    "width: 900px\n",
    "name: model-hub\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Benchmarks and Leaderboards: Chatbot Arena\n",
    "\n",
    "This is the [chatbot arena leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) as of 2024-03-04:\n",
    "\n",
    "```{figure} ./images/chatbot-leaderboard.png\n",
    "---\n",
    "width: 900px\n",
    "name: chatbot-leaderboard\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Benchmarks and Leaderboards: Others\n",
    "\n",
    "There are many [other benchmarks](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a):\n",
    "\n",
    "```{figure} ./images/big-benchmarks-collection.png\n",
    "---\n",
    "width: 900px\n",
    "name: big-benchmarks-collection\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Benchmarks and Leaderboards: HELM\n",
    "\n",
    "The growing capabilities of very large LLMs have inspired new and challenging benchmarks, like [HELM](https://crfm.stanford.edu/helm/lite/latest/):\n",
    "\n",
    "```{figure} ./images/helm-benchmark.png\n",
    "---\n",
    "width: 900px\n",
    "name: helm-benchmark\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{card} Executing an Open LLM\n",
    "\n",
    "Executing LLMs on a [GPU](https://blogs.nvidia.com/blog/whats-the-difference-between-a-cpu-and-a-gpu/) is __much__ faster than using CPU. We will show you how to access GPUs for training and inference on [Quest/KLC](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1112)\n",
    "\n",
    "```{figure} ./images/gpu-v-cpu.jpg\n",
    "---\n",
    "width: 900px\n",
    "name: gpu-v-cpu\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the Model: Fine-tuning\n",
    "\n",
    ":::{card}\n",
    "\n",
    "While we should always start with crafting good prompts in order to achieve the best performance we can, it may sometimes be advantageous to adapt a model to improve its performance. Fine-tuning is one way to achieve this goal.\n",
    "\n",
    "```{figure} ./images/project-lifecycle-4-annotated.png\n",
    "---\n",
    "width: 900px\n",
    "name: project-lifecycle-4\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Fine-tuning\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Evaluation Metrics\n",
    "\n",
    "Evaluation metrics depend on the type of task. For information extraction tasks, metrics such as [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) are appropriate\n",
    "\n",
    "```{figure} ./images/precision-recall.png\n",
    "---\n",
    "width: 400px\n",
    "float: left\n",
    "name: precision-recall\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Integration\n",
    "\n",
    ":::{card} \n",
    "LLMs are usually deployed as a component of a larger application. This larger application can make use of external resources, such as collections of documents, or knowledge bases. Deployment must also take into account the computational resources that are available, such as the availability of GPUs and sufficient memory.\n",
    "\n",
    "```{figure} ./images/project-lifecycle-5-annotated.png\n",
    "---\n",
    "width: 900px\n",
    "name: project-lifecycle-5\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Model Optimization\n",
    "Models can consume very large amounts of memory. The largest model you can currently run on Quest has to fit into a 4 Nvidia A100s with 80GB of RAM each. This is a lot, but you have to contend for these nodes with the rest of Northwestern. One way to tackle this challeng is to [quantize](https://huggingface.co/blog/4bit-transformers-bitsandbytes) your model weights, lowering FP precision in order to consume less memory:\n",
    "\n",
    "```{figure} ./images/FP8-scheme.png\n",
    "---\n",
    "width: 900px\n",
    "name: FP8-schema\n",
    "---\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Retrieval Augmented Generation (RAG)\n",
    "\n",
    "No model can \"know\" anything about events that have occurred after its training cutoff date. One way to overcome this obstacle is to integrate external resources, such as Retrieval Augmented Generation (RAG). RAG can result in better prompt completions and fewer \"hallucinations\".\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}