{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using llama_cpp_python to run LLMs\n",
    "`llama_cpp_python` is a Python wrapper for the C/C++ implementation of Meta's LLaMa architecture. It provides access to powerful open-source large language models (LLMs) built with the Llama-cpp framework, enabling tasks like text generation and translation.  For more information see this repo on llama.cpp (https://github.com/ggerganov/llama.cpp), this for the python wrapper (https://github.com/abetlen/llama-cpp-python), or this overview (https://www.datacamp.com/tutorial/llama-cpp-tutorial)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run llama_cpp_python, you can download any compatible model and model weights.  We provive sample code for 3 different models\n",
    "\n",
    "- (1.) Mistral\n",
    "- (2.) Gemma\n",
    "- (3.) Llama2\n",
    "\n",
    "Here is some sample code for the a gemma 7B instruct model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Gemma test code with llama_cpp\n",
    "#################################\n",
    "# libraries\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Inputs\n",
    "model_path =\"/model/gemma-7b-it.gguf\"\n",
    "CONTEXT_SIZE = 512\n",
    "temperature: float=0\n",
    "\n",
    "# basic prompt\n",
    "#prompt = \"Can you provide a summary of Guy Debord's Societe du Spectacle?\"\n",
    "\n",
    "# prompt written in gemma prompt syntax\n",
    "prompt = \"\"\"\n",
    "<start_of_turn>user\n",
    "Can you provide a summary of Guy Debord's Societe du Spectacle?\"<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# LOAD THE MODEL\n",
    "llm = Llama(\n",
    "  model_path=model_path,  # The path to the model file\n",
    "  n_ctx=CONTEXT_SIZE,  # The max sequence length to use - adjust based on your model's requirements\n",
    "  n_threads=1,  # The number of CPU threads to use\n",
    "  n_gpu_layers=-1  # Set to 0 if you want to use CPU only and -1 if you want to use all available GPUs\n",
    ")\n",
    "\n",
    "# send prompts\n",
    "response = llm(\"Can you provide a concise summary of Debord's Societe du Spectacle?\", max_tokens=1000, temperature=temperature)\n",
    "response_text = response['choices'][0]['text']\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this file is saved on KLC here:\n",
    "```\n",
    "/kelloggs/software/llama_cpp/code\n",
    "```\n",
    "\n",
    "This directory is bound to our llama-cpp-python model and can be accessed as:\n",
    "\n",
    "```\n",
    "/code\n",
    "```\n",
    "\n",
    "\n",
    "#### <font color='purple'>__SLURM Script to run llama_cpp_python__</font>\n",
    "\n",
    "You can run this file with the following SLURM script\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -A your_quest_allocation_account\n",
    "#SBATCH -p gengpu\n",
    "#SBATCH --gres=gpu:a100:1\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 1\n",
    "#SBATCH -t 0:30:00\n",
    "#SBATCH --mem=40G\n",
    "\n",
    "module purge\n",
    "module use modulefiles\n",
    "module load llama_cpp/2.38\n",
    "python3 /code/gemma_test.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, you can launch similar code for the mistral and llama2 models using the sample mistral_test.py and llama2_test.py files, respectively.  Each of these files are also found in the code subfolder."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}