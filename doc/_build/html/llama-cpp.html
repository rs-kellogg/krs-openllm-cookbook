

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Using Llama_cpp_python to run LLMs &#8212; Kellogg Research Support Open Source LLM Cookbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'llama-cpp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Example Use Case" href="use_case.html" />
    <link rel="prev" title="Using Transformers to run LLMs" href="transformers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="welcome.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Kellogg Research Support Open Source LLM Cookbook - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Kellogg Research Support Open Source LLM Cookbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Open Source Large Language Models (LLMs)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Running Open Source LLMs on KLC</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="slurm_gpu_usage.html">Using GPUs at Northwestern</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Using Transformers to run LLMs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Using Llama_cpp_python to run LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_case.html">Example Use Case</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Techniques</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="retrieval.html">Retrieval Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="fine-tuning.html">Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="vision.html">Image Processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="takeaways.html">Summary</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/rs-kellogg/krs-openllm-cookbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rs-kellogg/krs-openllm-cookbook/issues/new?title=Issue%20on%20page%20%2Fllama-cpp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/llama-cpp.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Using Llama_cpp_python to run LLMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-llama-cpp-python-vs-transformers-font"><font color="purple"><em>Llama_cpp_python vs. Transformers</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-basic-workflow-for-llama-cpp-python-font"><font color="purple"><em>Basic Workflow for <strong>llama_cpp_python</strong></em></font></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-quantized-models-what-is-gguf-font"><font color="purple"><strong>Quantized Models - What is GGUF?</strong></font></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-tradeoffs-between-tools-font"><font color="purple"><em>Tradeoffs between Tools</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-run-gemma-model-font"><font color="purple"><em>Run Gemma model</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-run-mistral-model-font"><font color="purple"><em>Run Mistral model</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-run-llama2-model-font"><font color="purple"><em>Run llama2 model</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-reference-sources-font"><font color="purple"><em>Reference Sources</em></font></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="using-llama-cpp-python-to-run-llms">
<h1>Using Llama_cpp_python to run LLMs<a class="headerlink" href="#using-llama-cpp-python-to-run-llms" title="Permalink to this heading">#</a></h1>
<p>Much like Tranformers, <code class="docutils literal notranslate"><span class="pre">LLaMa.cpp</span> <span class="pre">(or</span> <span class="pre">LLaMa</span> <span class="pre">C++)</span></code> is a tool or framework to run an open-source LLM. We’ll demonstrate how to use the Python wrapper for it: <font color='purple'><strong>llama_cpp_python</strong></font>.</p>
<section id="font-color-purple-llama-cpp-python-vs-transformers-font">
<h2><font color='purple'><em>Llama_cpp_python vs. Transformers</em></font><a class="headerlink" href="#font-color-purple-llama-cpp-python-vs-transformers-font" title="Permalink to this heading">#</a></h2>
<p>Both <font color='purple'><strong>Transformers</strong></font> and <font color='purple'><strong>llama_cpp_python</strong></font> are tools you can apply to run open-source LLMs. Since <code class="docutils literal notranslate"><span class="pre">LLaMa.cpp</span></code> is also derived from the Transformer architecture, they follow similar processing steps.  However, <code class="docutils literal notranslate"><span class="pre">LLaMa.cpp</span></code> applies some improvements that make it slightly more efficient and easier to run.  The diagram below details the difference between the Llama and Transformer architecture.</p>
<!-- <div style="text-align:center">
    <img src="./images/architectures.png" width="700">
</div> -->
<p><img alt="architects" src="_images/architectures.png" /></p>
<p>The upshot of these tweaks are:</p>
<ul class="simple">
<li><p><strong>Better training</strong>: LLaMA uses a technique called <strong>pre-normalization</strong> to improves training stability by adjusting the data before processing it. Consider this akin to tuning up your guitar before each song to ensure the strings are at the right tension for consistent sound.</p></li>
</ul>
<!-- This is similar to how a chef might preheat an oven for even cooking. -->
<ul class="simple">
<li><p><strong>Faster calculations</strong>: LLaMA utilizes the <strong>SwiGLU activation function</strong> to help the model compute information faster. This is like using a pick with a special grip that makes it easier and faster to strum chords or play single notes.</p></li>
</ul>
 <!-- This is like a sharper knife for quicker chopping. -->
<ul class="simple">
<li><p><strong>Simpler positioning</strong>: LLaMA incorporates <strong>rotary embeddings</strong> which track word order in a more efficient way compared to absolute positional embeddings. Imagine that the frets at different positions of your guitar neck were color-coded instead of just metal bars. This makes it quicker to find the right notes on the fretboard.</p></li>
</ul>
 <!-- Imagine using color-coded ingredients (rotary) instead of labels (absolute) to keep track in a recipe--></section>
<section id="font-color-purple-basic-workflow-for-llama-cpp-python-font">
<h2><font color='purple'><em>Basic Workflow for <strong>llama_cpp_python</strong></em></font><a class="headerlink" href="#font-color-purple-basic-workflow-for-llama-cpp-python-font" title="Permalink to this heading">#</a></h2>
<p>In addition to the architectural improvements, <font color='purple'><strong>llama_cpp_python</strong></font> is simpler to use than the python transformers library. Unlike transformers, you don’t need to be familiar with libraries like PyTorch or the intricacies of token conversion and decoding. <font color='purple'><strong>Llama_cpp_python</strong></font> takes care of these technical details under the hood, allowing you to focus on feeding it text and getting results directly.</p>
<p><strong>1. Load the model:</strong>
Load the model in a <font color='purple'><strong>GGUF</strong></font> format.</p>
<p><strong>2. Send your prompt:</strong>
Either feed it a plain text prompt or place the query in the appropriate model prompt syntax.</p>
<p><strong>3. Get text response:</strong>
Retrieve the text response with no decoding necessary.</p>
<section id="font-color-purple-quantized-models-what-is-gguf-font">
<h3><font color='purple'><strong>Quantized Models - What is GGUF?</strong></font><a class="headerlink" href="#font-color-purple-quantized-models-what-is-gguf-font" title="Permalink to this heading">#</a></h3>
<p>In order to run an LLM, <font color='purple'><strong>llama_cpp_python</strong></font> requires the model’s parameters – the building blocks of its knowledge. These are stored as complex, multidimensional arrays of values called <strong>tensors</strong>. In machine learning a tensor shows the values of nodes in a neural network.</p>
<p><img alt="tensor" src="_images/tensor.png" /></p>
<p>The data type chosen for these tensors (like Float64, Float16, or even integers) impacts both accuracy and memory usage. You can think of the data type as the number of “digits” used to represent the information in memory:</p>
<ul class="simple">
<li><p><strong>Higher precision data types (e.g., Float64)</strong>: Offer greater accuracy and stability during training, but require more memory and computational resources.</p></li>
<li><p><strong>Lower precision data types (e.g., Float16)</strong>: Reduce memory requirements and potentially speed up computations, but might introduce slight accuracy trade-offs.</p></li>
</ul>
<p>Llama_cpp_python simplifies how to store model inputs by using a single, compressed format called <font color='purple'><strong>GGUF (GPT-Generated Unified Format)</strong></font>. <font color='purple'><strong>GGUF</strong></font> can leverage a technique called <font color='purple'><strong>quantization</strong></font> to further optimize storage. <font color='purple'><strong>Quantization</strong></font> essentially “compresses” the data by representing the model weights using less precise data types. The image below demonstrates how <font color='purple'><strong>quantization</strong></font> works:</p>
<!--<div style="text-align:center">
    <img src="./images/quantize.gif" width="800">
</div> -->
<p><img alt="quantize" src="_images/quantize.gif" /></p>
<p>In practice <font color='purple'><strong>quantization</strong></font> can entail moving from a float 16 (FP16) data type to an integer 4 (INT4) data type.</p>
<p>By using <font color='purple'><strong>quantization</strong></font>, <font color='purple'><strong>llama_cpp_python</strong></font> makes it easier to work with LLMs on devices with limited memory or processing power. Here you see how quantizaton reduces the size of the llama2 models you can call.  This makes it feasible to run these models on a single GPU or even CPUs.</p>
<!--<div style="text-align:center">
    <img src="./images/llama_size.png" width="800">
</div>-->
<p><img alt="llama" src="_images/llama_size.png" /></p>
<p>While you can create your own <font color='purple'><strong>GGUF</strong></font> files for most models, it’s also possible to download these files from Hugging Face. The <font color='purple'><strong>GGUF</strong></font> files for llama2 can be found here: <a class="reference external" href="https://huggingface.co/TheBloke/Llama-2-7B-GGUF">https://huggingface.co/TheBloke/Llama-2-7B-GGUF</a>.</p>
</section>
</section>
<section id="font-color-purple-tradeoffs-between-tools-font">
<h2><font color='purple'><em>Tradeoffs between Tools</em></font><a class="headerlink" href="#font-color-purple-tradeoffs-between-tools-font" title="Permalink to this heading">#</a></h2>
<p>Even though the architecture and basic steps suggest that <font color='purple'><strong>llama_cpp_python</strong></font> might be easier to run and more efficient than the python transformer library, the improvements come with some tradeoffs.</p>
<ul class="simple">
<li><p><strong>Inference only</strong>: llama_cpp_python is primarily for running inference (using the model to generate text). You can’t fine-tune the model (further train it on a specific task) or use techniques like RAG (using a retrieval model to improve factual accuracy) through llama_cpp_python. Transformers offer more comprehensive functionality for model development and experimentation.</p></li>
<li><p><strong>Slower for small models</strong>: llama_cpp_python might be slower for very small GGUF files. The overhead of using llama_cpp_python might outweigh the benefits for tiny models. Transformers might be a better choice in such cases.</p></li>
<li><p><strong>Precision loss</strong>: Quantization (converting complex numbers to simpler formats) does introduce some loss in precision. This might be negligible for many tasks, but for applications requiring very high accuracy, transformers with full-precision computation could be preferable.</p></li>
</ul>
<p><font color='purple'><em>Choose <strong>Transformers</strong> for:</em></font></p>
<ul class="simple">
<li><p>Fine-tuning models</p></li>
<li><p>Using RAG or other advanced techniques</p></li>
<li><p>Working with very small models</p></li>
<li><p>When high precision is crucial</p></li>
</ul>
<p><font color='purple'><em>Choose <strong>llama_cpp_python</strong> for:</em></font></p>
<ul class="simple">
<li><p>Faster inference on large models</p></li>
<li><p>Deploying models on CPUs</p></li>
</ul>
</section>
<section id="font-color-purple-run-gemma-model-font">
<h2><font color='purple'><em>Run Gemma model</em></font><a class="headerlink" href="#font-color-purple-run-gemma-model-font" title="Permalink to this heading">#</a></h2>
<p>Here is some sample code to run a single prompt with the gemma 7B instruct model:</p>
<p><font color='purple'><strong>Python script</strong></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">###########################################</span>
<span class="c1"># Gemma model example with llama_cpp_python</span>
<span class="c1">###########################################</span>
<span class="c1"># libraries</span>
<span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>

<span class="c1">#########</span>
<span class="c1"># Inputs</span>
<span class="c1">#########</span>

<span class="c1"># model</span>
<span class="n">model_path</span> <span class="o">=</span><span class="s2">&quot;/kellogg/software/llama_cpp/model/gemma-7b-it.gguf&quot;</span>

<span class="c1"># settings</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">threads</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">gpu_layers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">max_tokens_select</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">temperature_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">0</span>
<span class="n">top_p_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span>
<span class="n">top_k_select</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
<span class="n">include_prompt</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Prompts</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;What kind of pickups are on an ESP LTD Alexi Ripped?&quot;</span>

<span class="c1"># Prompt Syntax (prompt written in gemma prompt syntax)</span>
<span class="n">prompt_sytnax</span> <span class="o">=</span> <span class="s2">&quot;&lt;start_of_turn&gt;user&quot;</span> <span class="o">+</span> <span class="n">prompt</span> <span class="o">+</span> <span class="s2">&quot;&lt;end_of_turn&gt;&quot;</span> <span class="o">+</span> <span class="s2">&quot;&lt;start_of_turn&gt;model&quot;</span>

<span class="c1">#####################</span>
<span class="c1"># LLaMa.cpp arguments</span>
<span class="c1">#####################</span>

<span class="c1"># load model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span>
  <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>  <span class="c1"># The path to the model file</span>
  <span class="n">n_ctx</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>  <span class="c1"># The max sequence length to use - adjust based on your model&#39;s requirements</span>
  <span class="n">n_threads</span><span class="o">=</span><span class="n">threads</span><span class="p">,</span>  <span class="c1"># The number of CPU threads to use</span>
  <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">gpu_layers</span>  <span class="c1"># Set to 0 if you want to use CPU only and -1 if you want to use all available GPUs</span>
<span class="p">)</span>

<span class="c1"># send prompt</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens_select</span><span class="p">,</span> 
    <span class="n">temperature</span><span class="o">=</span><span class="n">temperature_select</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="n">top_p_select</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="n">top_k_select</span><span class="p">,</span>
    <span class="n">echo</span> <span class="o">=</span> <span class="n">include_prompt</span>
    <span class="p">)</span>

<span class="c1">##############</span>
<span class="c1"># Get Response</span>
<span class="c1">##############</span>
<span class="n">response_text</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Breaking down the parameter options:</p>
<p><strong>At the model load step</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_path</span></code> is the path to the model file being used. Note that you can download this file from Hugging Face here: <a class="reference external" href="https://huggingface.co/google/gemma-7b-it/tree/main">https://huggingface.co/google/gemma-7b-it/tree/main</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_ctx</span></code> is the context window for the model; max number of tokens in the prompt; default is 512</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_threads</span></code> is the number of the number of CPU threads</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_gpu_layers</span></code>set to 0 if you want to use CPU only and -1 if you want to use all available GPUs</p></li>
</ul>
<p><strong>At the model output step</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prompt</span></code> is the input prompt for the model. Under the hood, the text is tokenized and passed to the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code> is the maximum number of tokens to be generated in the model’s response</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code> this value ranges from 0 to 1. The lower the value, the more deterministic the end result. A higher value leads to more randomness.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_p</span></code> is used to control the diversity of the predictions, meaning that it selects the most probable tokens whose cumulative probability exceeds a given threshold. Starting from zero, a higher value increases the chance of finding a better output but requires additional computations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">echo</span></code> specifies a boolean used to determine whether the model includes the original prompt at the beginning (True) or does not include it (False).</p></li>
</ul>
<p><em>For more information about these parameters and additional ones, please see: <a class="reference external" href="https://llama-cpp-python.readthedocs.io/en/latest/api-reference/">https://llama-cpp-python.readthedocs.io/en/latest/api-reference/</a>.</em></p>
<p>You can find this file here: <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/gemma_ex.py">scripts/llama_cpp_python/gemma_ex.py</a>.  This file is saved as <strong>gemma_ex.py</strong> on KLC here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">kelloggs</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">llama_cpp</span><span class="o">/</span><span class="n">code</span>
</pre></div>
</div>
<p><font color='purple'><strong>SLURM script</strong></font></p>
<p>You can run this file with the following SLURM script:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --account=e32337</span>
<span class="c1">#SBATCH --partition gengpu</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --gres=gpu:a100:1</span>
<span class="c1">#SBATCH --time 0:30:00</span>
<span class="c1">#SBATCH --mem=40G</span>

<span class="n">module</span> <span class="n">purge</span>
<span class="n">module</span> <span class="n">use</span> <span class="o">/</span><span class="n">kellogg</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">Modules</span><span class="o">/</span><span class="n">modulefiles</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">llama_cpp</span><span class="o">/</span><span class="mf">2.38</span>

<span class="n">python3</span> <span class="n">gemma_ex</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
</div>
<p>You can find this file here: <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/gemma_ex.sh">scripts/llama_cpp_python/gemma_ex.sh</a>. This file is also saved to the same directory provided above as <strong>gemma_test.sh</strong>. You can run this shell script by making it an executable then launching it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chmod</span> <span class="o">+</span><span class="n">x</span> <span class="n">gemma_ex</span><span class="o">.</span><span class="n">sh</span>
<span class="o">./</span><span class="n">gemma_test</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ggml_init_cublas</span><span class="p">:</span> <span class="n">no</span> <span class="n">CUDA</span> <span class="n">devices</span> <span class="n">found</span><span class="p">,</span> <span class="n">CUDA</span> <span class="n">will</span> <span class="n">be</span> <span class="n">disabled</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="n">loaded</span> <span class="n">meta</span> <span class="n">data</span> <span class="k">with</span> <span class="mi">19</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">pairs</span> <span class="ow">and</span> <span class="mi">254</span> <span class="n">tensors</span> <span class="kn">from</span> <span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">gemma</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">it</span><span class="o">.</span><span class="n">gguf</span> <span class="p">(</span><span class="n">version</span> <span class="n">GGUF</span> <span class="n">V3</span> <span class="p">(</span><span class="n">latest</span><span class="p">))</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="n">Dumping</span> <span class="n">metadata</span> <span class="n">keys</span><span class="o">/</span><span class="n">values</span><span class="o">.</span> <span class="n">Note</span><span class="p">:</span> <span class="n">KV</span> <span class="n">overrides</span> <span class="n">do</span> <span class="ow">not</span> <span class="n">apply</span> <span class="ow">in</span> <span class="n">this</span> <span class="n">output</span><span class="o">.</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">0</span><span class="p">:</span>                       <span class="n">general</span><span class="o">.</span><span class="n">architecture</span> <span class="nb">str</span>              <span class="o">=</span> <span class="n">gemma</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">1</span><span class="p">:</span>                               <span class="n">general</span><span class="o">.</span><span class="n">name</span> <span class="nb">str</span>              <span class="o">=</span> <span class="n">gemma</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">it</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">2</span><span class="p">:</span>                       <span class="n">gemma</span><span class="o">.</span><span class="n">context_length</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">8192</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">3</span><span class="p">:</span>                          <span class="n">gemma</span><span class="o">.</span><span class="n">block_count</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">28</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">4</span><span class="p">:</span>                     <span class="n">gemma</span><span class="o">.</span><span class="n">embedding_length</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">3072</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">5</span><span class="p">:</span>                  <span class="n">gemma</span><span class="o">.</span><span class="n">feed_forward_length</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">24576</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">6</span><span class="p">:</span>                 <span class="n">gemma</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">head_count</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">16</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">7</span><span class="p">:</span>              <span class="n">gemma</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">head_count_kv</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">16</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">8</span><span class="p">:</span>                 <span class="n">gemma</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">key_length</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">256</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>   <span class="mi">9</span><span class="p">:</span>               <span class="n">gemma</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">value_length</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">256</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">10</span><span class="p">:</span>     <span class="n">gemma</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">layer_norm_rms_epsilon</span> <span class="n">f32</span>              <span class="o">=</span> <span class="mf">0.000001</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">11</span><span class="p">:</span>                       <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">model</span> <span class="nb">str</span>              <span class="o">=</span> <span class="n">llama</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">12</span><span class="p">:</span>                <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">2</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">13</span><span class="p">:</span>                <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">1</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">14</span><span class="p">:</span>            <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">padding_token_id</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">0</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">15</span><span class="p">:</span>            <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">unknown_token_id</span> <span class="n">u32</span>              <span class="o">=</span> <span class="mi">3</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">16</span><span class="p">:</span>                      <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">tokens</span> <span class="n">arr</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span><span class="mi">256128</span><span class="p">]</span>  <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;eos&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;bos&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span> <span class="o">...</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">17</span><span class="p">:</span>                      <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">scores</span> <span class="n">arr</span><span class="p">[</span><span class="n">f32</span><span class="p">,</span><span class="mi">256128</span><span class="p">]</span>  <span class="o">=</span> <span class="p">[</span><span class="mf">0.000000</span><span class="p">,</span> <span class="mf">0.000000</span><span class="p">,</span> <span class="mf">0.000000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="o">...</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="n">kv</span>  <span class="mi">18</span><span class="p">:</span>                  <span class="n">tokenizer</span><span class="o">.</span><span class="n">ggml</span><span class="o">.</span><span class="n">token_type</span> <span class="n">arr</span><span class="p">[</span><span class="n">i32</span><span class="p">,</span><span class="mi">256128</span><span class="p">]</span>  <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">...</span>
<span class="n">llama_model_loader</span><span class="p">:</span> <span class="o">-</span> <span class="nb">type</span>  <span class="n">f32</span><span class="p">:</span>  <span class="mi">254</span> <span class="n">tensors</span>
<span class="n">llm_load_vocab</span><span class="p">:</span> <span class="n">mismatch</span> <span class="ow">in</span> <span class="n">special</span> <span class="n">tokens</span> <span class="n">definition</span> <span class="p">(</span> <span class="mi">544</span><span class="o">/</span><span class="mi">256128</span> <span class="n">vs</span> <span class="mi">388</span><span class="o">/</span><span class="mi">256128</span> <span class="p">)</span><span class="o">.</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="nb">format</span>           <span class="o">=</span> <span class="n">GGUF</span> <span class="n">V3</span> <span class="p">(</span><span class="n">latest</span><span class="p">)</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">arch</span>             <span class="o">=</span> <span class="n">gemma</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">vocab</span> <span class="nb">type</span>       <span class="o">=</span> <span class="n">SPM</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_vocab</span>          <span class="o">=</span> <span class="mi">256128</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_merges</span>         <span class="o">=</span> <span class="mi">0</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_ctx_train</span>      <span class="o">=</span> <span class="mi">8192</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_embd</span>           <span class="o">=</span> <span class="mi">3072</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_head</span>           <span class="o">=</span> <span class="mi">16</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_head_kv</span>        <span class="o">=</span> <span class="mi">16</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_layer</span>          <span class="o">=</span> <span class="mi">28</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_rot</span>            <span class="o">=</span> <span class="mi">192</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_embd_head_k</span>    <span class="o">=</span> <span class="mi">256</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_embd_head_v</span>    <span class="o">=</span> <span class="mi">256</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_gqa</span>            <span class="o">=</span> <span class="mi">1</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_embd_k_gqa</span>     <span class="o">=</span> <span class="mi">4096</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_embd_v_gqa</span>     <span class="o">=</span> <span class="mi">4096</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">f_norm_eps</span>       <span class="o">=</span> <span class="mf">0.0e+00</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">f_norm_rms_eps</span>   <span class="o">=</span> <span class="mf">1.0e-06</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">f_clamp_kqv</span>      <span class="o">=</span> <span class="mf">0.0e+00</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">f_max_alibi_bias</span> <span class="o">=</span> <span class="mf">0.0e+00</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_ff</span>             <span class="o">=</span> <span class="mi">24576</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_expert</span>         <span class="o">=</span> <span class="mi">0</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_expert_used</span>    <span class="o">=</span> <span class="mi">0</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">rope</span> <span class="n">scaling</span>     <span class="o">=</span> <span class="n">linear</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">freq_base_train</span>  <span class="o">=</span> <span class="mf">10000.0</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">freq_scale_train</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">n_yarn_orig_ctx</span>  <span class="o">=</span> <span class="mi">8192</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">rope_finetuned</span>   <span class="o">=</span> <span class="n">unknown</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">model</span> <span class="nb">type</span>       <span class="o">=</span> <span class="mi">7</span><span class="n">B</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">model</span> <span class="n">ftype</span>      <span class="o">=</span> <span class="nb">all</span> <span class="n">F32</span> <span class="p">(</span><span class="n">guessed</span><span class="p">)</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">model</span> <span class="n">params</span>     <span class="o">=</span> <span class="mf">8.54</span> <span class="n">B</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">model</span> <span class="n">size</span>       <span class="o">=</span> <span class="mf">31.81</span> <span class="n">GiB</span> <span class="p">(</span><span class="mf">32.00</span> <span class="n">BPW</span><span class="p">)</span> 
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">general</span><span class="o">.</span><span class="n">name</span>     <span class="o">=</span> <span class="n">gemma</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">it</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">BOS</span> <span class="n">token</span>        <span class="o">=</span> <span class="mi">2</span> <span class="s1">&#39;&lt;bos&gt;&#39;</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">EOS</span> <span class="n">token</span>        <span class="o">=</span> <span class="mi">1</span> <span class="s1">&#39;&lt;eos&gt;&#39;</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">UNK</span> <span class="n">token</span>        <span class="o">=</span> <span class="mi">3</span> <span class="s1">&#39;&lt;unk&gt;&#39;</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">PAD</span> <span class="n">token</span>        <span class="o">=</span> <span class="mi">0</span> <span class="s1">&#39;&lt;pad&gt;&#39;</span>
<span class="n">llm_load_print_meta</span><span class="p">:</span> <span class="n">LF</span> <span class="n">token</span>         <span class="o">=</span> <span class="mi">227</span> <span class="s1">&#39;&lt;0x0A&gt;&#39;</span>
<span class="n">llm_load_tensors</span><span class="p">:</span> <span class="n">ggml</span> <span class="n">ctx</span> <span class="n">size</span> <span class="o">=</span>    <span class="mf">0.10</span> <span class="n">MiB</span>
<span class="n">llm_load_tensors</span><span class="p">:</span> <span class="n">offloading</span> <span class="mi">28</span> <span class="n">repeating</span> <span class="n">layers</span> <span class="n">to</span> <span class="n">GPU</span>
<span class="n">llm_load_tensors</span><span class="p">:</span> <span class="n">offloading</span> <span class="n">non</span><span class="o">-</span><span class="n">repeating</span> <span class="n">layers</span> <span class="n">to</span> <span class="n">GPU</span>
<span class="n">llm_load_tensors</span><span class="p">:</span> <span class="n">offloaded</span> <span class="mi">29</span><span class="o">/</span><span class="mi">29</span> <span class="n">layers</span> <span class="n">to</span> <span class="n">GPU</span>
<span class="n">llm_load_tensors</span><span class="p">:</span>        <span class="n">CPU</span> <span class="n">buffer</span> <span class="n">size</span> <span class="o">=</span> <span class="mf">32570.17</span> <span class="n">MiB</span>
<span class="o">............................................................................................</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span> <span class="n">n_ctx</span>      <span class="o">=</span> <span class="mi">512</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span> <span class="n">freq_base</span>  <span class="o">=</span> <span class="mf">10000.0</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span> <span class="n">freq_scale</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">WARNING</span><span class="p">:</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">allocate</span> <span class="mf">224.00</span> <span class="n">MB</span> <span class="n">of</span> <span class="n">pinned</span> <span class="n">memory</span><span class="p">:</span> <span class="n">no</span> <span class="n">CUDA</span><span class="o">-</span><span class="n">capable</span> <span class="n">device</span> <span class="ow">is</span> <span class="n">detected</span>
<span class="n">llama_kv_cache_init</span><span class="p">:</span>        <span class="n">CPU</span> <span class="n">KV</span> <span class="n">buffer</span> <span class="n">size</span> <span class="o">=</span>   <span class="mf">224.00</span> <span class="n">MiB</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span> <span class="n">KV</span> <span class="bp">self</span> <span class="n">size</span>  <span class="o">=</span>  <span class="mf">224.00</span> <span class="n">MiB</span><span class="p">,</span> <span class="n">K</span> <span class="p">(</span><span class="n">f16</span><span class="p">):</span>  <span class="mf">112.00</span> <span class="n">MiB</span><span class="p">,</span> <span class="n">V</span> <span class="p">(</span><span class="n">f16</span><span class="p">):</span>  <span class="mf">112.00</span> <span class="n">MiB</span>
<span class="n">WARNING</span><span class="p">:</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">allocate</span> <span class="mf">8.01</span> <span class="n">MB</span> <span class="n">of</span> <span class="n">pinned</span> <span class="n">memory</span><span class="p">:</span> <span class="n">no</span> <span class="n">CUDA</span><span class="o">-</span><span class="n">capable</span> <span class="n">device</span> <span class="ow">is</span> <span class="n">detected</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span>        <span class="n">CPU</span> <span class="nb">input</span> <span class="n">buffer</span> <span class="n">size</span>   <span class="o">=</span>     <span class="mf">8.01</span> <span class="n">MiB</span>
<span class="n">WARNING</span><span class="p">:</span> <span class="n">failed</span> <span class="n">to</span> <span class="n">allocate</span> <span class="mf">506.25</span> <span class="n">MB</span> <span class="n">of</span> <span class="n">pinned</span> <span class="n">memory</span><span class="p">:</span> <span class="n">no</span> <span class="n">CUDA</span><span class="o">-</span><span class="n">capable</span> <span class="n">device</span> <span class="ow">is</span> <span class="n">detected</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span>  <span class="n">CUDA_Host</span> <span class="n">compute</span> <span class="n">buffer</span> <span class="n">size</span> <span class="o">=</span>   <span class="mf">506.25</span> <span class="n">MiB</span>
<span class="n">llama_new_context_with_model</span><span class="p">:</span> <span class="n">graph</span> <span class="n">splits</span> <span class="p">(</span><span class="n">measure</span><span class="p">):</span> <span class="mi">1</span>
<span class="n">AVX</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">AVX_VNNI</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">AVX2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">AVX512</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">AVX512_VBMI</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">AVX512_VNNI</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">FMA</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">NEON</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">ARM_FMA</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">F16C</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">FP16_VA</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">WASM_SIMD</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">BLAS</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">SSE3</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">SSSE3</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">|</span> <span class="n">VSX</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> <span class="n">MATMUL_INT8</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">|</span> 
<span class="n">Model</span> <span class="n">metadata</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;tokenizer.ggml.unknown_token_id&#39;</span><span class="p">:</span> <span class="s1">&#39;3&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer.ggml.padding_token_id&#39;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer.ggml.eos_token_id&#39;</span><span class="p">:</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;general.architecture&#39;</span><span class="p">:</span> <span class="s1">&#39;gemma&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.feed_forward_length&#39;</span><span class="p">:</span> <span class="s1">&#39;24576&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.attention.head_count&#39;</span><span class="p">:</span> <span class="s1">&#39;16&#39;</span><span class="p">,</span> <span class="s1">&#39;general.name&#39;</span><span class="p">:</span> <span class="s1">&#39;gemma-7b-it&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.context_length&#39;</span><span class="p">:</span> <span class="s1">&#39;8192&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.block_count&#39;</span><span class="p">:</span> <span class="s1">&#39;28&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.embedding_length&#39;</span><span class="p">:</span> <span class="s1">&#39;3072&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.attention.head_count_kv&#39;</span><span class="p">:</span> <span class="s1">&#39;16&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.attention.key_length&#39;</span><span class="p">:</span> <span class="s1">&#39;256&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer.ggml.model&#39;</span><span class="p">:</span> <span class="s1">&#39;llama&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.attention.value_length&#39;</span><span class="p">:</span> <span class="s1">&#39;256&#39;</span><span class="p">,</span> <span class="s1">&#39;gemma.attention.layer_norm_rms_epsilon&#39;</span><span class="p">:</span> <span class="s1">&#39;0.000001&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer.ggml.bos_token_id&#39;</span><span class="p">:</span> <span class="s1">&#39;2&#39;</span><span class="p">}</span>

<span class="n">llama_print_timings</span><span class="p">:</span>        <span class="n">load</span> <span class="n">time</span> <span class="o">=</span>    <span class="mf">1047.55</span> <span class="n">ms</span>
<span class="n">llama_print_timings</span><span class="p">:</span>      <span class="n">sample</span> <span class="n">time</span> <span class="o">=</span>     <span class="mf">111.74</span> <span class="n">ms</span> <span class="o">/</span>    <span class="mi">34</span> <span class="n">runs</span>   <span class="p">(</span>    <span class="mf">3.29</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">token</span><span class="p">,</span>   <span class="mf">304.29</span> <span class="n">tokens</span> <span class="n">per</span> <span class="n">second</span><span class="p">)</span>
<span class="n">llama_print_timings</span><span class="p">:</span> <span class="n">prompt</span> <span class="nb">eval</span> <span class="n">time</span> <span class="o">=</span>    <span class="mf">1047.37</span> <span class="n">ms</span> <span class="o">/</span>    <span class="mi">15</span> <span class="n">tokens</span> <span class="p">(</span>   <span class="mf">69.82</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">token</span><span class="p">,</span>    <span class="mf">14.32</span> <span class="n">tokens</span> <span class="n">per</span> <span class="n">second</span><span class="p">)</span>
<span class="n">llama_print_timings</span><span class="p">:</span>        <span class="nb">eval</span> <span class="n">time</span> <span class="o">=</span>  <span class="mf">136433.05</span> <span class="n">ms</span> <span class="o">/</span>    <span class="mi">33</span> <span class="n">runs</span>   <span class="p">(</span> <span class="mf">4134.33</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">token</span><span class="p">,</span>     <span class="mf">0.24</span> <span class="n">tokens</span> <span class="n">per</span> <span class="n">second</span><span class="p">)</span>
<span class="n">llama_print_timings</span><span class="p">:</span>       <span class="n">total</span> <span class="n">time</span> <span class="o">=</span>  <span class="mf">138373.07</span> <span class="n">ms</span> <span class="o">/</span>    <span class="mi">48</span> <span class="n">tokens</span>


<span class="n">The</span> <span class="n">Esp</span> <span class="n">Ltd</span><span class="o">.</span><span class="n">AlexI</span> <span class="n">Ripper</span> <span class="n">has</span> <span class="n">ceramic</span> <span class="n">humbuckers</span> <span class="k">with</span> <span class="n">coil</span> <span class="n">tapping</span> <span class="n">capabilities</span><span class="p">,</span> <span class="n">which</span> <span class="n">give</span> <span class="n">you</span> <span class="n">a</span> <span class="n">wide</span> <span class="nb">range</span> <span class="kn">from</span> <span class="nn">clean</span> <span class="n">to</span> <span class="n">overdrive</span> <span class="n">tones</span> <span class="ow">and</span> <span class="n">everything</span> <span class="ow">in</span> <span class="n">between</span>

</pre></div>
</div>
<p>Likewise, you can modify the python file to run multiple prompts.  The example script below is saved as <strong>gemma_workflow.py</strong> in the same directory listed above.  You can also find it here: <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/gemma_workflow.py">scripts/llama_cpp_python/gemma_workflow.py</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">######################################</span>
<span class="c1"># Gemma Workflow with llama_cpp_python</span>
<span class="c1">######################################</span>
<span class="c1"># libraries</span>
<span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1">#########</span>
<span class="c1"># Inputs</span>
<span class="c1">#########</span>

<span class="c1"># model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;/kellogg/software/llama_cpp/models/gemma-7b-it.gguf&quot;</span>

<span class="c1"># prompts</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;What is the main idea of Guy Debord&#39;s Societe du Spectacle?&quot;</span><span class="p">,</span> 
           <span class="s2">&quot;What kind of pickups are on an ESP LTD Alexi Ripped?&quot;</span><span class="p">,</span> 
           <span class="s2">&quot;How does Allama Iqbal&#39;s concept of the khudi relate to Nietzsche&#39;s Ubermensch?&quot;</span>
<span class="p">]</span>

<span class="c1"># output</span>
<span class="n">output_file</span> <span class="o">=</span> <span class="s2">&quot;/kellogg/software/llama_cpp/output/gemma_test.csv&quot;</span>

<span class="c1"># settings</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># The max sequence length to use - adjust based on your model&#39;s requirements</span>
<span class="n">threads</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># The number of CPU threads to use</span>
<span class="n">gpu_layers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># Set to 0 if you want to use CPU only and -1 if you want to use all available GPUs</span>
<span class="n">max_tokens_select</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">temperature_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">0</span> 
<span class="n">top_p_select</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.9</span>
<span class="n">top_k_select</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
<span class="n">include_prompt</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1">############</span>
<span class="c1"># Functions</span>
<span class="c1">############</span>

<span class="c1"># get prompt response</span>
<span class="k">def</span> <span class="nf">get_completion</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens_select</span><span class="p">,</span> <span class="n">temperature_select</span><span class="p">,</span> <span class="n">top_p_select</span><span class="p">,</span> <span class="n">top_k_select</span><span class="p">,</span> <span class="n">include_prompt</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>

    <span class="c1"># send prompt</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span>
      <span class="n">prompt</span><span class="p">,</span>
      <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens_select</span><span class="p">,</span> 
      <span class="n">temperature</span><span class="o">=</span><span class="n">temperature_select</span><span class="p">,</span>
      <span class="n">top_p</span><span class="o">=</span><span class="n">top_p_select</span><span class="p">,</span>
      <span class="n">top_k</span><span class="o">=</span><span class="n">top_k_select</span><span class="p">,</span>
      <span class="n">echo</span> <span class="o">=</span> <span class="n">include_prompt</span><span class="p">)</span>
    
    <span class="c1"># get response</span>
    <span class="n">response_text</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">response_text</span>
  
  <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="c1"># save results to a df</span>
<span class="k">def</span> <span class="nf">save_results</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">run_time</span><span class="p">):</span>

    <span class="c1"># create empty df</span>
    <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">,</span> <span class="s1">&#39;response&#39;</span><span class="p">,</span> <span class="s1">&#39;run_time&#39;</span><span class="p">])</span>
    
    <span class="c1"># create df from current row</span>
    <span class="n">row_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;prompt&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">prompt</span><span class="p">],</span>
        <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">],</span>
        <span class="s1">&#39;run_time&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">run_time</span><span class="p">]</span>
    <span class="p">})</span>
    
    <span class="c1"># combine</span>
    <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">row_df</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># return dataframe</span>
    <span class="k">return</span> <span class="n">results_df</span>

<span class="c1">######</span>
<span class="c1"># RUN</span>
<span class="c1">######</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
   <span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>  
               <span class="n">n_ctx</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>  
               <span class="n">n_threads</span><span class="o">=</span><span class="n">threads</span><span class="p">,</span>  
               <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">gpu_layers</span><span class="p">)</span>

   <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
        <span class="c1"># run</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">max_tokens_select</span><span class="p">,</span> <span class="n">temperature_select</span><span class="p">,</span> <span class="n">top_p_select</span><span class="p">,</span> <span class="n">top_k_select</span><span class="p">,</span> <span class="n">include_prompt</span><span class="p">)</span>
        <span class="n">run_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

        <span class="c1"># print results</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;========================&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run Time: </span><span class="si">{</span><span class="n">run_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;========================&quot;</span><span class="p">)</span>

        <span class="c1"># save progress</span>
        <span class="n">results_df</span> <span class="o">=</span> <span class="n">save_results</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">run_time</span><span class="p">)</span>
        <span class="n">results_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>.....
llama_print_timings:        load time =    5117.72 ms
llama_print_timings:      sample time =     910.97 ms /   320 runs   (    2.85 ms per token,   351.28 tokens per second)
llama_print_timings: prompt eval time =    5116.70 ms /    18 tokens (  284.26 ms per token,     3.52 tokens per second)
llama_print_timings:        eval time =   85380.73 ms /   319 runs   (  267.65 ms per token,     3.74 tokens per second)
llama_print_timings:       total time =   96673.56 ms /   337 tokens
========================
Prompt: What is the main idea of Guy Debord&#39;s Societe du Spectacle?
Response: 

The Society of Control (1973) argues that in advanced capitalist societies, social life itself has become a spectacle. This society rests on two pillars: control and simulation—the latter being more powerful than even economic power because it controls our perception as individuals as well as our understanding as collective beings.&lt;br&gt; 
Guy debord argued against this view claiming instead for an understanding rooted firmly within historical materialism.&lt;br&gt; 
Debord argued against seeing contemporary capitalism as primarily concerned with production for profit but rather with production for consumption.&lt; He argued this shift was evident not only from changes to labor practices but also from changes to consumer behavior.&lt;br&gt; 
Therefore his primary concern was not with individual perception but with collective behavior.&lt; He argued this shift was evident not only from changes to labor practices but also from changes to consumer behavior.&lt;br&gt; 
Run Time: 96.674813747406
========================

llama_print_timings:        load time =    5117.72 ms
llama_print_timings:      sample time =     107.74 ms /    34 runs   (    3.17 ms per token,   315.57 tokens per second)
llama_print_timings: prompt eval time =    1299.31 ms /    13 tokens (   99.95 ms per token,    10.01 tokens per second)
llama_print_timings:        eval time =    8821.49 ms /    33 runs   (  267.32 ms per token,     3.74 tokens per second)
llama_print_timings:       total time =   10758.35 ms /    46 tokens

========================
Prompt: What kind of pickups are on an ESP LTD Alexi Ripped?
Response: 

The Esp Ltd.AlexI Ripper has ceramic humbuckers with coil tapping capabilities, which give you a wide range from clean to overdrive tones and everything in between
Run Time: 10.760140419006348

========================

llama_print_timings:        load time =    5268.39 ms
llama_print_timings:      sample time =    1060.79 ms /   377 runs   (    2.81 ms per token,   355.40 tokens per second)
llama_print_timings: prompt eval time =    1910.34 ms /    20 tokens (   95.52 ms per token,    10.47 tokens per second)
llama_print_timings:        eval time =   99700.63 ms /   376 runs   (  265.16 ms per token,     3.77 tokens per second)
llama_print_timings:       total time =  108664.66 ms /   396 tokens
========================
Prompt: How does Allama Iqbal&#39;s concept of the khudi relate to Nietzsche&#39;s Ubermensch?
Response: 

Allama MuhammadIqbal and FriedrichNietzsche were two prominent philosophers who explored similar themes in their respective works. One such theme is that  of self-cultivation, which they referred as Khudī (Urdu) or Übermenschen(German). 


**Khüdî by Allaima IQBAL:**
In his poetry collection &quot;The Secrets,&quot; Allaima IQBAL argued for a spiritual awakening through individual effort: 
&quot;Khüdî hai har insān ka apna,&quot; meaning &quot;Selfhood&quot; or &quot;Individuality&quot; for every human being.&quot; 
He believed individuals should cultivate inner strength through moral development rather than material possessions or external validation . 


**Übermenschen by NIETZSCHE:**
Friedrich NIETZSCHE proposed a different approach towards selffactualization: through overcoming limitations rather than conforming with societal norms:
&quot;The Übermenschen... are those individuals whose spiritual strength has overcome all limitations.&quot;
He emphasized individual strength over conformity , urging humans not be afraid challenge existing structures .


**Comparison:**
Both concepts emphasize individual empowerment through spiritual awakening or overcoming limitations . However , there are some key differences between them:
* **Emphasis:**
 - **IQBAL:** Focuses on moral development through individual effort
 - **NIETZSCHE:** Emphasizes spiritual strength through overcoming limitations
* **Approach:**
 - **IQBAL:** Advocates for spiritual awakening through moral development
 - **NIETZSCHE:** Encourages overcoming limitations through individual strength
* **Cultural Context:**
 - **IQBAL:** Rooted within South Asian cultural values
 - **NIETZSCHE:** Developed within European philosophical tradition

**Conclusion:**
While both concepts share similarities , each philosopher offers a unique perspective on individual empowerment through spiritual awakening or overcoming limitations . IQBAL emphasizes moral development within a cultural context , while NIETZSCHE encourages individual strength beyond societal norms .
Run Time: 108.66637086868286

</pre></div>
</div>
</section>
<section id="font-color-purple-run-mistral-model-font">
<h2><font color='purple'><em>Run Mistral model</em></font><a class="headerlink" href="#font-color-purple-run-mistral-model-font" title="Permalink to this heading">#</a></h2>
<p>Please see <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/mistral_ex.py">scripts/llama_cpp_python/misral_ex.py</a>.</p>
<p>Note that you can download this file from Hugging Face here: <a class="reference external" href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF">https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF</a></p>
</section>
<section id="font-color-purple-run-llama2-model-font">
<h2><font color='purple'><em>Run llama2 model</em></font><a class="headerlink" href="#font-color-purple-run-llama2-model-font" title="Permalink to this heading">#</a></h2>
<p>Please see <a class="reference external" href="https://github.com/rs-kellogg/krs-openllm-cookbook/blob/main/scripts/llama_cpp_python/llama2_ex.py">scripts/llama_cpp_python/llama2_ex.py</a>.</p>
<p>Note that you can download this file from Hugging Face here:  <a class="reference external" href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF">https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF</a></p>
</section>
<section id="font-color-purple-reference-sources-font">
<h2><font color='purple'><em>Reference Sources</em></font><a class="headerlink" href="#font-color-purple-reference-sources-font" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/ggerganov/llama.cpp">Llama.cpp Git Repo</a></p></li>
<li><p><a class="reference external" href="https://github.com/abetlen/llama-cpp-python">Llama.cpp.python Git Repo</a></p></li>
<li><p><a class="reference external" href="https://www.datacamp.com/tutorial/llama-cpp-tutorial">Llama.cpp Tuturial</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;metechsolutions/llm-by-examples-use-gguf-quantization-3e2272b66343">LLM By Examples — Use GGUF Quantization</a></p></li>
<li><p><a class="reference external" href="https://www.tensorops.ai/post/what-are-quantized-llms">What are Quantized LLMs</a></p></li>
<li><p><a class="reference external" href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html">Quantize Llama Models with GGUF adn llama.cpp</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;ingridwickstevens/quantization-of-llms-with-llama-cpp-9bbf59deda35">Quantization of LLMs with llama.cpp</a></p></li>
<li><p><a class="reference external" href="https://www.promptingguide.ai/">Prompt Engineering Guide</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="transformers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Using Transformers to run LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="use_case.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Example Use Case</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-llama-cpp-python-vs-transformers-font"><font color="purple"><em>Llama_cpp_python vs. Transformers</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-basic-workflow-for-llama-cpp-python-font"><font color="purple"><em>Basic Workflow for <strong>llama_cpp_python</strong></em></font></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-quantized-models-what-is-gguf-font"><font color="purple"><strong>Quantized Models - What is GGUF?</strong></font></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-tradeoffs-between-tools-font"><font color="purple"><em>Tradeoffs between Tools</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-run-gemma-model-font"><font color="purple"><em>Run Gemma model</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-run-mistral-model-font"><font color="purple"><em>Run Mistral model</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-run-llama2-model-font"><font color="purple"><em>Run llama2 model</em></font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#font-color-purple-reference-sources-font"><font color="purple"><em>Reference Sources</em></font></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kellogg Research Support
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>