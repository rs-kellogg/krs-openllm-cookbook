{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation  \n",
    "\n",
    "<font color='purple'>Retrieval Augmented Generation (RAG)</font> is a powerful paradigm in natural language processing that combines the strengths of information retrieval and language generation. This approach involves retrieving relevant information from a large dataset and using that information to enhance the generation of accurate text.  \n",
    "\n",
    "The phrase <font color='purple'>Retrieval Augmented Generation</font> comes from a recent [paper by Lewis et al. from Facebook AI](https://research.facebook.com/publications/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks/). The idea is to use a pre-trained language model (LM) to generate text, but to use a separate retrieval system to find relevant documents to condition the LM on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Sample Uses Cases</font>\n",
    "\n",
    "- **Question Answering Systems or Conversational Agents**:  \n",
    "Retrieve information from vast knowledge bases, such as multiple pdf or csv files, and incorporate into response.\n",
    "\n",
    "- **Context Creation**:  \n",
    "Enhance the generation of informative text by pulling in relevant details from a wide range of sources.\n",
    "\n",
    "- **Code Generation**:  \n",
    "Assist in generating code snippets by retrieving information from programming knowledge bases. \n",
    "\n",
    "- **Prevent Hallucinations**:  \n",
    "Bring in external knowledge to check whether a GPT response is a hallucination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>How it Works</font>\n",
    "\n",
    "From start to finish, the RAG relies on 5 steps:\n",
    "![RAG steps](./images/rag.png)  \n",
    "\n",
    "**1. Load**  \n",
    "Load documents from different source files (_url, csv, pdf, txt_) in diverse locations (s3 storage, public sites, etc.)  \n",
    "\n",
    "**2. Transform**  \n",
    "Prepare larger documents for retrieval by creating splits or chunks the data.  \n",
    "\n",
    "**3. Embed**  \n",
    "Create embeddings for documents to capture the semantic meaning of the text. This later enables models to efficiently find other pieces of text that are similar.  \n",
    "\n",
    "**4. Store**  \n",
    "Vector stores support efficient storage and search of document embeddings.  \n",
    "\n",
    "**5. Retrieve**  \n",
    "Relevant information is retrieved to produce more informed and context-aware responses.\n",
    "\n",
    "During runtime, this blending of retrieval and generation enhances the richness and relevance of the generated content.\n",
    "![RAG runtime](./images/basic_rag.png)  \n",
    "\n",
    "_Taken from: [https://docs.llamaindex.ai/en/stable/_static/getting_started/basic_rag.png]([https://docs.llamaindex.ai/en/stable/_static/getting_started/basic_rag.png])_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Example: Retrieving Information Non-existent in Training</font>\n",
    "\n",
    "One way to use RAG is to feed the LLM with up-to-date information.  \n",
    "The Llama 2 was trained between January 2023 and July 2023. The Mistral 7B model was released in September 2023. Let's ask LLama2 a question about the Mistral 7B model.\n",
    "\n",
    "```\n",
    "# Run the query through Llama2 7B chat model with test_llama2.py  \n",
    "query = \"[INST]How can I use tranformers to call the Mistral 7B instruct model?[/INST]\"\n",
    "```\n",
    "\n",
    "__Output:__  \n",
    "> [INST]How can I use tranformers to call the Mistral 7B instruct model?[/INST]  \n",
    "> To use Transformers to call the Mistral 7B instruct model, you can follow these general steps:\n",
    "> \n",
    "> 1. Install the necessary dependencies: You will need to install the `transformers` library and any other dependencies required by the model you want to use.\n",
    "> 2. Load the model: You can use the `transformers.BertModel` class to load the Mistral 7B model. For example:\n",
    "> ```\n",
    "> from transformers import BertModel\n",
    "> \n",
    "> model = BertModel.from_pretrained('mistral-7b-base-uncased')\n",
    "> ```\n",
    "> 3. Prepare the input: You will need to prepare the input for the model. This can involve tokenizing the text, padding it to the appropriate length, and converting it to a format that the model can understand.\n",
    "> 4. Call the model: You can use the `model.predict()` method to call the model on a given input. For example:\n",
    "> ```\n",
    "> input_text = 'This is a sample input text.'\n",
    "> input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "> attention_mask = tokenizer.encode(input_text, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "> \n",
    "> output = model(input_ids, attention_mask)\n",
    "> ```\n",
    "> 5. Interpret the output: Once you have called the model, you can interpret the output in a way that makes sense for your use case. For example, you could use the output to generate a summary of the input text, or to perform some other task that requires understanding natural language.\n",
    "> \n",
    "> Note that the specific details of how to use the Mistral 7B model will depend on the implementation of the model and the library you are using. You may need to consult the documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface page for Mistral 7B Instruct model can be found [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2).  \n",
    "We can use RAG to feed up-to-date information. The code below will take the contents of a webpage and follow the 5 steps outlined above to retrieve the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Save text from the url link\n",
    "def prep_text(workdir, url_link):\n",
    "    response = requests.get(url_link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    webpage_content = soup.get_text().strip()\n",
    "    webpage_content = os.linesep.join([s for s in webpage_content.splitlines() if s])\n",
    "\n",
    "    data_path = workdir / \"LlamaIndex_data\"\n",
    "    if not data_path.exists():\n",
    "        data_path.mkdir()\n",
    "\n",
    "    txt_file = data_path / \"webpage_content.txt\"\n",
    "    with open(txt_file, \"w\") as f:\n",
    "        f.write(webpage_content)\n",
    "\n",
    "# Perform RAG using LlamaIndex\n",
    "def process_llamaindex(workdir, embedding_name, embed_d, llm_model, query):\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "    embedding_chunk_size = 1024\n",
    "    embed_model = HuggingFaceEmbedding(model_name=embedding_name, max_length=512)\n",
    "\n",
    "    llm = HuggingFaceLLM(\n",
    "        context_window=4096,\n",
    "        max_new_tokens=1000,\n",
    "        tokenizer_name=llm_model,\n",
    "        model_name=llm_model,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    PERSIST_DIR = workdir / \"LlamaIndex_storage_faiss\"\n",
    "    data_path = workdir / \"LlamaIndex_data\"\n",
    "\n",
    "    if not PERSIST_DIR.exists():\n",
    "        print(\"== Processing data ... \")\n",
    "\n",
    "        ##############################\n",
    "        # 1. Load the documents\n",
    "        ##############################\n",
    "        documents = SimpleDirectoryReader(data_path).load_data()\n",
    "\n",
    "        # Set up FAISS vector store\n",
    "        faiss_index = faiss.IndexFlatL2(embed_d)\n",
    "        vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, \n",
    "            ##############################\n",
    "            # 2. Transform\n",
    "            ##############################\n",
    "            transformations=[SentenceSplitter(chunk_size=embedding_chunk_size)],\n",
    "            ##############################\n",
    "            # 3. Embed\n",
    "            ##############################\n",
    "            embed_model=embed_model, \n",
    "            ##############################\n",
    "            # 4. Store\n",
    "            ##############################\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "        print(\"== Finish saving vector store data ... \")\n",
    "\n",
    "    else:\n",
    "        print(\"== Storage exists. Loading data ...\")\n",
    "\n",
    "        vector_store = FaissVectorStore.from_persist_dir(PERSIST_DIR)\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            persist_dir=PERSIST_DIR,\n",
    "            vector_store=vector_store, \n",
    "        )\n",
    "        index = load_index_from_storage(\n",
    "            transformations=[SentenceSplitter(chunk_size=embedding_chunk_size)],\n",
    "            embed_model=embed_model, \n",
    "            storage_context=storage_context,\n",
    "            )\n",
    "        print(\"== Finish loading ...\")\n",
    "\n",
    "    ##############################\n",
    "    # 5. Retrieve\n",
    "    ##############################\n",
    "    query_engine = index.as_query_engine(llm = llm)\n",
    "\n",
    "    print(f\"Selected LLM: {llm_model}\")\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"Response: \")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "# Replace your_file_directory with your project directory string\n",
    "file_dir = your_file_directory\n",
    "workdir = Path(file_dir)\n",
    "url_link = \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "prep_text(workdir, url_link)\n",
    "\n",
    "\n",
    "# Settings for embedding model\n",
    "# embedding_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "embedding_name = \"/kellogg/data/llm_models_opensource/e5_infloat/models--intfloat--multilingual-e5-large-instruct/snapshots/baa7be480a7de1539afce709c8f13f833a510e0a\"\n",
    "embed_d = 1024 # embedding dimension\n",
    "\n",
    "# Settings for LLM\n",
    "LLAMA2_7B_CHAT = \"/kellogg/data/llm_models_opensource/llama2_meta_huggingface/models--meta-llama--Llama-2-7b-chat-hf/snapshots/92011f62d7604e261f748ec0cfe6329f31193e33\"\n",
    "# LLAMA2_13B_CHAT = \"/kellogg/data/llm_models_opensource/llama2_meta_huggingface/models--meta-llama--Llama-2-13b-chat-hf/snapshots/29655417e51232f4f2b9b5d3e1418e5a9b04e80e\"\n",
    "llm_model = LLAMA2_7B_CHAT\n",
    "# llm_model = LLAMA2_13B_CHAT\n",
    "\n",
    "query = \"[INST]How can I use tranformers to call the Mistral 7B instruct model?[/INST]\"\n",
    "\n",
    "process_llamaindex(workdir, embedding_name, embed_d, llm_model, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output:__  \n",
    "> Selected LLM: /kellogg/data/llm_models_opensource/llama2_meta_huggingface/models--meta-llama--Llama-2-7b-chat-hf/snapshots/92011f62d7604e261f748ec0cfe6329f31193e33  \n",
    ">   \n",
    "> Batches:   0%|          | 0/1 [00:00<?, ?it/s]  \n",
    "> Batches: 100%|██████████| 1/1 [00:00<00:00, 63.51it/s]  \n",
    ">   \n",
    "> Query: [INST]How can I use tranformers to call the Mistral 7B instruct model?[/INST]  \n",
    "> Response:   \n",
    ">   \n",
    "> To use transformers to call the Mistral 7B Instruct model, you can follow these steps:  \n",
    ">   \n",
    "> 1. Install the transformers library by running `pip install transformers` in your terminal.  \n",
    "> 2. Import the AutoModelForCausalLM and AutoTokenizer classes from the transformers library:  \n",
    "> ```python  \n",
    "> from transformers import AutoModelForCausalLM, AutoTokenizer  \n",
    "> ```  \n",
    "> 3. Load the Mistral 7B Instruct model using the `from_pretrained` method:  \n",
    "> ```python  \n",
    "> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")  \n",
    "> ```  \n",
    "> 4. Use the `apply_chat_template` method to apply the instruction format to a given set of messages:  \n",
    "> ```python  \n",
    "> messages = [  \n",
    ">     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},  \n",
    ">     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},  \n",
    ">     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}  \n",
    "> ]  \n",
    ">   \n",
    "> encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")  \n",
    "> model_inputs = encodeds.to(device)  \n",
    "> model.to(device)  \n",
    "> generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)  \n",
    "> decoded = tokenizer.batch_decode(generated_ids)  \n",
    "> print(decoded[0])  \n",
    "> ```  \n",
    "> 5. Fine-tune the model by adjusting the hyperparameters or adding additional data to improve its performance.  \n",
    ">   \n",
    "> Note: The `mistralai` name is a placeholder for the actual model name, which you can replace with the name of your own model. Also, make sure to install the transformers library by running `pip install transformers` in your terminal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>Sample scripts</font>  \n",
    "Sample scripts can be found at the [scripts/rag](https://github.com/rs-kellogg/krs-openllm-cookbook/tree/main/scripts/rag) folder of our github repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
