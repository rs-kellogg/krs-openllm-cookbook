{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use Transformers to run LLMs\n",
    "\n",
    "## Basic workflow for using transformers\n",
    "<font color='red'>1. Load the model parameters</font>  \n",
    "<font color='orange'>2. Convert prompt query to tokens</font>  \n",
    "<font color='darkblue'>3. Call model to process tokens and generate response tokens</font>  \n",
    "<font color='green'>4. Decode tokens to text response</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Llama2 model with transformers  \n",
    "Model on the Hugging Face site: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf  \n",
    "### Python script\n",
    "__# test_llama2.py__  \n",
    "  \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  \n",
    "from transformers import BitsAndBytesConfig  \n",
    "import time  \n",
    "import pandas as pd  \n",
    "from pathlib import Path  \n",
    "  \n",
    "start_time = time.time()  \n",
    "\n",
    "_# Set up model and directory info_    \n",
    "llm_dir = \"/kellogg/data/llm_models_opensource/llama2_meta_huggingface\"  \n",
    "llm_model = \"meta-llama/Llama-2-7b-chat-hf\"  \n",
    "_# llm_model = \"meta-llama/Llama-2-13b-chat-hf\"_  \n",
    "_# llm_model = \"meta-llama/Llama-2-70b-chat-hf\"_  \n",
    "  \n",
    "<font color='red'>quantization_config = BitsAndBytesConfig(load_in_8bit=True)</font>   \n",
    "<font color='red'>model = AutoModelForCausalLM.from_pretrained(llm_model,cache_dir=llm_dir, device_map=\"auto\", quantization_config=quantization_config)</font>   \n",
    "<font color='red'>tokenizer = AutoTokenizer.from_pretrained(llm_model, cache_dir=llm_dir)</font>   \n",
    "  \n",
    "print(f\"=== Loading time: {time.time() - start_time} seconds\")  \n",
    "  \n",
    "_# For Llama2 chat, need to enclosed your prompt by [INST] and [/INST]_  \n",
    "query = \"[INST] Tell a fun fact about Kellogg Business School. [/INST]\"  \n",
    "  \n",
    "<font color='orange'>device = \"cuda\"</font>  \n",
    "<font color='orange'>model_input = tokenizer(query, return_tensors=\"pt\").to(device)</font>  \n",
    "\n",
    "_# Settings for LLM model_    \n",
    "customize_setting = {  \n",
    "    \"max_new_tokens\": 400,  \n",
    "    \"do_sample\": True,  \n",
    "    \"temperature\": 0.8,  \n",
    "}  \n",
    "print(f\"=== Customized setting:\")  \n",
    "for key, value in customize_setting.items():  \n",
    "    print(f\"    {key}: {value}\")  \n",
    "  \n",
    "<font color='darkblue'>outputs = model.generate(**model_input, **customize_setting)</font>  \n",
    "<font color='green'>decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)</font>  \n",
    "  \n",
    "print(\"====================\")  \n",
    "print(f\"LLM model: {llm_model}\")  \n",
    "print(f\"Query: {query}\")  \n",
    "print(\"Response: \")  \n",
    "print(decoded)  \n",
    "print(\"====================\")  \n",
    "  \n",
    "end_time = time.time()  \n",
    "execution_time = end_time - start_time  \n",
    "print(f\"Execution time: {execution_time} seconds\")  \n",
    "finished_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())  \n",
    "print(f\"Finished at: {finished_time}\")  \n",
    "print(\"====================\")  \n",
    "  \n",
    "_# Logging_  \n",
    "columns = [\"llm_model\", \"query\", \"response\", \"finished_time\"]  \n",
    "row = [llm_model, query, decoded, finished_time]  \n",
    "for key, value in customize_setting.items():  \n",
    "    columns.append(key)  \n",
    "    row.append(value)  \n",
    "df = pd.DataFrame([row], columns=columns)  \n",
    "llm_name = llm_model.split(\"/\")[-1]  \n",
    "log_file = Path(f\"./log_{llm_name}.csv\")  \n",
    "df.to_csv(log_file, index=False, mode='a', header=not log_file.exists())  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slurm script\n",
    "The following script is for running the GPU job on Quest. To run with Kellogg GPU, change the \"#SBATCH -A\" and \"#SBATCH -p\" lines to  \n",
    "```  \n",
    "#SBATCH -A kellogg\n",
    "#SBATCH -p kellogg\n",
    "```  \n",
    "__# run_batch_llama2.sh__  \n",
    "```\n",
    "# Run on Quest\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -A your_quest_allocation_account\n",
    "#SBATCH -p gengpu\n",
    "#SBATCH --gres=gpu:a100:1\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 1\n",
    "#SBATCH -t 0:30:00\n",
    "#SBATCH --mem=40G\n",
    "\n",
    "module purge\n",
    "module load mamba/23.1.0\n",
    "source /hpc/software/mamba/23.1.0/etc/profile.d/conda.sh\n",
    "source activate /kellogg/software/envs/gpu-llama2\n",
    "\n",
    "python test_llama2.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Mistral model with transformers  \n",
    "Check out scripts/test_mistral.py.  \n",
    "\n",
    "## Run Gemma model with transformers  \n",
    "Check out scripts/tset_gemma.py.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
