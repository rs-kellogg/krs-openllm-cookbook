{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>__KLC Containers__</font>\n",
    "Much like __conda environments__, <font color='purple'>__containers__</font> enable you to package software and dependencies.  \n",
    "\n",
    "### <font color='purple'>__Why Containers?__</font>\n",
    "\n",
    "While __conda environments__ are best suited for managing Python/R packages and their related libraries, they fall short for larger projects that require non-Python dependencies. However, <font color='purple'>__containers__</font> __shine__ when you need to: \n",
    "\n",
    "\n",
    "- safeguard your project environment __against admin updates/changes__ to KLC/Quest or accidental environment modifications by collaborators\n",
    "\n",
    "- install something that __requires admin or super user privileges__ (apt-get, or yum) that we don't have on KLC/Quest\n",
    "\n",
    "- __transfer your complete workflow__ (via a *container image*) from machine to machine (KLC to your computer; MIT server to KLC, etc.) \n",
    "\n",
    "- create virtual machines that run on a __completely different operating system__ (Windows, MacOS, Ubuntu) than KLC/Quest (RHEL7)\n",
    "\n",
    "This guide demonstrates how to install __llama_cpp_python__ on KLC using <font color='purple'>__containers__</font>. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: __llama_cpp_python__ is a Python wrapper for the C/C++ implementation of Meta's LLaMa architecture. For more information see this repo on llama.cpp (https://github.com/ggerganov/llama.cpp), this for the python wrapper (https://github.com/abetlen/llama-cpp-python), or this overview (https://www.datacamp.com/tutorial/llama-cpp-tutorial)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide: \n",
    "\n",
    "- 1.) summarizes the __terminology__ and steps used when working with containers; \n",
    "\n",
    "- 2.) demonstrates how to copy a <font color='purple'>__Docker image__</font> to use on KLC for __llama_cpp_python__ that runs on CPUs-only.\n",
    "\n",
    "- 3.) walks through how to install llama_cpp_python that runs on GPUs by:\n",
    "     \n",
    "     - a.) <font color='purple'>__Building__</font> a <font color='purple'>__Docker image__</font> that runs on KLC/Quest's GPU nodes\n",
    "     \n",
    "     - b.) <font color='purple'>__Pushing__</font> this <font color='purple'>__Docker image__</font> to <font color='purple'>__Docker hub__</font> to access it from KLC\n",
    "     \n",
    "     - c.) <font color='purple'>__Building__</font> a <font color='purple'>__Singularity image__</font> from your underlying <font color='purple'>__Docker image__</font>\n",
    "     \n",
    "     - d.) Running that <font color='purple'>__Singularity image__</font> on our KLC GPU node (that has only 1 GPU) and a Quest GPU node (multiple GPUs are available through the SLURM scheduler).\n",
    "\n",
    "- 4.) saves a singularity image as a KLC __module__ to easily access in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>1.) __Terminology and Steps__</font>\n",
    "\n",
    "Here are some key terms to know when working with containers: \n",
    "\n",
    "- <font color='purple'>__Images__</font>: blueprints or templates that define the contents and configuration of a container. You can think of this as the _blueprint_ containing all the components and instructions to build a house.\n",
    "\n",
    "- <font color='purple'>__Build__</font>: the process of creating a <font color='purple'>__container image__</font> from a set of instructions (a <font color='purple'>__Dockerfile__</font>) or <font color='purple'>__Singularity recipe file__</font>.  Think of this as _following a blueprint_ to construct a house. _Note that containers cannot be modified after the `build' step._\n",
    "\n",
    "- <font color='purple'>__Push__</font>: uploading a <font color='purple'>__container image__</font> built locally to a public registry where it can be accesssed and shared.  This is like moving a completed house (<font color='purple'>__container image__</font>) to a neighborhood, like (<font color='purple'>__Docker Hub__</font>) for others to see.\n",
    "\n",
    "- <font color='purple'>__Pull__</font>: downloading a <font color='purple'>__container image__</font> from a registry to another system. We can pull a <font color='purple'>__docker image__</font> from Docker Hub to KLC/Quest.\n",
    "\n",
    "This guide leverages both <font color='purple'>__Singularity__</font> and <font color='purple'>__Docker__</font>.  While there are multiple differences between the two platforms, our use-case comes down to:\n",
    "\n",
    "-  adopting <font color='purple'>__Docker__</font> for containers in non-HPC environments.  For instance, we don't have sudo privileges on KLC, so we need to create container images on our own computers.\n",
    "\n",
    "- leveraging <font color='purple'>__Singularity__</font> for containers in HPC environments, like KLC/Quest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='purple'>2.) __Access llama_cpp_python (CPU) Docker image on KLC__</font>\n",
    "\n",
    "The easiest way to create a <font color='purple'>__Singularity image__</font> is from an existing <font color='purple'>__Docker image__</font>.  For instance, there are many images available for llama_cpp_python (cpu-only) on Docker Hub.  One example is the image we created here: https://hub.docker.com/r/rskellogg/llama-cpp-python/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rs_llama.png\" width=\"700\" height=\"450\">\n",
    "<!-- ![alt text](/Users/ambreenchaudhri/Desktop/new_tools/llama_cpp_python/rs_llama.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate this to a Singularity image on KLC, run: \n",
    "\n",
    "``` \n",
    "module purge\n",
    "module load singularity\n",
    "singularity pull docker://rskellogg/llama-cpp-python:latest\n",
    "```\n",
    "_Please note that these images must be available in a public Docker Hub container.  Also, please specify any tags for the image._\n",
    "\n",
    "\n",
    "Running this command will create a singularity image file (as shown below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](sif_cpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run a Singularity container in a few ways:\n",
    "- (1) from inside an interactive __shell__; \n",
    "- (2) through an __exec__ or __run__ command (depending on how the script was written.)\n",
    "\n",
    "#### <font color='purple'>_Interactive Shell_</font>\n",
    "\n",
    "To run the llama_cppy_python (CPU-only) in an interactive shell, type:\n",
    "\n",
    "``` \n",
    "singularity shell /kellogg/software/simages/llama-cpp-python.sif\n",
    "```\n",
    "In order to access a space other than your home directory from inside your singularity image, you will need to \"Bind\" that directory to your image.  Here is an example of binding two directories (one with the model, another with the code) to my singularity image:\n",
    "\n",
    "```\n",
    "singularity shell -B /kellogg/software/llama_cpp/models:/model -B /kellogg/software/llama_cpp/code:/code /kellogg/software/simages/llama-cpp-python.sif\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run llama_cpp_python, you can download any compatible model and model weights.  For instance, different mistral models can be found here: https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF\n",
    "The script below uses this model: __mistral-7b-v0.1.Q5_K_S.gguf__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Llama_cpp_python test code\n",
    "############################\n",
    "\n",
    "# libraries\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Inputs\n",
    "my_model_path = \"/model/mistral-7b-v0.1.Q5_K_S.gguf\"\n",
    "CONTEXT_SIZE = 512\n",
    "\n",
    "# LOAD THE MODEL\n",
    "llm = Llama(\n",
    "  model_path=my_model_path,  # The path to the model file\n",
    "  n_ctx=CONTEXT_SIZE,  # The max sequence length to use - adjust based on your model's requirements\n",
    "  n_threads=4,  # The number of CPU threads to use\n",
    "  n_gpu_layers=0  # Set to 0 if you want to use CPU only and -1 if you want to use all available GPUs\n",
    ")\n",
    "\n",
    "# send prompts\n",
    "response = llm(\"Can you provide a concise summary of Debord's Societe du Spectacle?\", max_tokens=1000)\n",
    "response_text = response['choices'][0]['text']\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is saved as __llama_cpp_python_test.py__ in your /kellogg/software/llama_cpp/code.  You can launch this code inside the shell with: \n",
    "\n",
    "``` \n",
    "python3 /code/llama_cpp_test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>_Execute Command_</font>\n",
    "\n",
    "We can launch a script from inside the singularity container without entering the singularity shell by either using \"singularity exec\" or \"singularity run\". To use a \"singularity run\" command, the appropriate \"runscript\" sections of the Dockerfile or singularity recipe would need to be created. Whereas, we can launch \"singularity exec\" on any Singularity image.\n",
    "\n",
    "This command will run the same llama test file using \"singularity exec\":\n",
    "\n",
    "``` \n",
    "singularity exec -B /kellogg/software/llama_cpp/models:/model -B /kellogg/software/llama_cpp/code:/code /kellogg/software/simages/llama-cpp-python.sif python3 /code/llama_cpp_test.py\n",
    "```\n",
    "\n",
    "_If you need more guidance on binding directories, please see NUIT's explanation of \"Binding Directories\" for singularity use here: https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=1748._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>3.) __Create and Run a llama_cpp_python GPU Image on KLC__</font>\n",
    "\n",
    "In order to create a llama_cpp_python image that runs on both GPUs and CPUs, we will need to create a Docker image that accounts for the architecture of our GPU nodes and KLC and Quest.  We can start by \n",
    "by cloning the llama_cpp_python git repo:\n",
    "\n",
    "```\n",
    "git clone --depth 1 --branch main https://github.com/abetlen/llama-cpp-python\n",
    "```\n",
    "\n",
    "The following Dockerfile enables us to build a Docker Image for llama_cpp_python using Cuda 12.1 on our GPU nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "```\n",
    "ARG CUDA_IMAGE=\"12.1.1-devel-ubuntu22.04\"\n",
    "FROM nvidia/cuda:${CUDA_IMAGE}\n",
    "\n",
    "# We need to set the host to 0.0.0.0 to allow outside access\n",
    "ENV HOST 0.0.0.0\n",
    "\n",
    "RUN apt-get update && apt-get upgrade -y \\\n",
    "    && apt-get install -y git build-essential \\\n",
    "    python3 python3-pip gcc wget \\\n",
    "    ocl-icd-opencl-dev opencl-headers clinfo \\\n",
    "    libclblast-dev libopenblas-dev \\\n",
    "    && mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n",
    "\n",
    "# setting build related env vars\n",
    "ENV CUDA_DOCKER_ARCH=all\n",
    "ENV LLAMA_CUBLAS=1\n",
    "\n",
    "# Install depencencies\n",
    "RUN python3 -m pip install --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette pydantic-settings starlette-context\n",
    "\n",
    "# added a line for additional packages\n",
    "RUN python3 -m pip install numpy pandas parmap nltk statsmodels scipy\n",
    "RUN python3 -m pip install huggingface_hub notebook jupyterlab\n",
    "RUN python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "RUN python3 -m pip install 'transformers[torch]'\n",
    "\n",
    "RUN git clone https://github.com/NVIDIA/apex\n",
    "WORKDIR /apex\n",
    "# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... \n",
    "RUN git checkout 23.08\n",
    "RUN python3 -m pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
    "\n",
    "RUN rm -rf /apex\n",
    "WORKDIR /\n",
    "\n",
    "# Install llama-cpp-python (build with cuda)\n",
    "RUN CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
    "\n",
    "#RUN python3 -m pip cache remove\n",
    "\n",
    "COPY . .\n",
    "\n",
    "# Run the server\n",
    "CMD python3 -m llama_cpp.server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please save this file locally on your computer. Then use it to replace the existing Dockerfile in your cloned llama_cpp_python folder: \n",
    "\n",
    "```\n",
    "# overwrite the cuda_simple Dockerfile with our Dockerfile\n",
    "cp Dockerfile llama-cpp-python/docker/cuda_simple/Dockerfile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, please make sure you have Docker Desktop on your computer and an active Docker account.  You can now build your Docker container with: \n",
    "\n",
    "```\n",
    "# build the container\n",
    "cd llama-cpp-python \n",
    "docker build -f docker/cuda_simple/Dockerfile -t llama:<llama-cpp-version> . --platform=linux/amd64\n",
    "```\n",
    "Please insert the llama-cpp version here: __&lt;llama-cpp-version&gt;__ like this:\n",
    "\n",
    "```\n",
    "docker build -f docker/cuda_simple/Dockerfile -t llama:v0.2.38 . --platform=linux/amd64\n",
    "```\n",
    "\n",
    "Once the build step is complete, please login to Dockerhub.  Then you can tag and push your docker image to Docker Hub with:\n",
    "\n",
    "```\n",
    "# Log in to Dockerhub\n",
    "docker login\n",
    "\n",
    "# Tag the Docker image\n",
    "docker tag llama:v0.2.38 ambreenwchaudhri/llama:v0.2.38\n",
    "\n",
    "# Push the Docker image to Dockerhub\n",
    "docker push ambreenwchaudhri/llama:v0.2.38\n",
    "```\n",
    "\n",
    "Please replace __ambreenwchaudhri__ with your Docker Hub username. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Docker Image is built and publicly available on Dockerhub, you can move to KLC and create a Singularity image there.\n",
    "\n",
    "Please sign into any KLC node.  From the command line, type:\n",
    "\n",
    "```\n",
    "module load singularity/3.8.1\n",
    "singularity pull docker://ambreenwchaudhri/llama-cpp-python:v0.2.38\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>4.) __Save an Image as a KLC Module__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this singularity image, you will need to access a GPU node on KLC or Quest. \n",
    "To use an interactive session on our GPU node, login to any KLC node and type the following:\n",
    "\n",
    "```\n",
    "srun --partition=kellogg --account=kellogg --nodes=1 --ntasks-per-node=1 --gres=gpu:a100:1 --mem=20G --time=05:30:00 --pty bash -l\n",
    "srun --partition=gengpu --account=p30790 --nodes=1 --ntasks-per-node=1 --gres=gpu:a100:1 --mem=20G --time=05:30:00 --pty bash -l\n",
    "```\n",
    "\n",
    "Once you are on the GPU node, you can run the same test file through an interactive shell session with the following: \n",
    "\n",
    "```\n",
    "module load singularity/3.8.1\n",
    "singularity shell -B /kellogg/software/llama_cpp/models:/model -B /kellogg/software/llama_cpp/code:/code /kellogg/software/llama_cpp/llama_v0.2.38.sif\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.1/compat\n",
    "python3 /code/llama_cpp_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since calling singularity containers requires so many steps, we can package all of these steps into a KLC/Quest module using a .lua file.  Within /kellogg/software/modulefiles/llama_cpp, I created a .lua file called 2.38.lua that looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# new lua file\n",
    "depends_on(\"singularity/3.8.1\")\n",
    "local python3_str = 'singularity exec --nv -B /kellogg/software/llama_cpp/models:/model -B /kellogg/software/llama_cpp/code:/code --env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.1/compat /kellogg/software/llama_cpp/llama_v0.2.38.sif python3 $@'\n",
    "set_shell_function(\"python3\", python3_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .lua file translates all of our singularity container calling steps into a KLC model that we can access with the following steps:\n",
    "\n",
    "```\n",
    "module purge\n",
    "module use modulefiles\n",
    "module load llama_cpp/2.38\n",
    "```\n",
    "\n",
    "Once the module is loaded, you can simply login to a GPU node and launch your code file with:\n",
    "\n",
    "```\n",
    "python3 /code/llama_cpp_test.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
